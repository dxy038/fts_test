In this paper a particle swarm optimization (PSO)-based training strategy is introduced for fuzzy ARTMAP that minimizes generalization error while optimizing parameter values. Through a comprehensive set simulations, it has been shown that this training strategy allows fuzzy ARTMAP to achieve a significantly lower generalization error than when it uses typical training strategies. Furthermore, the PSO strategy eliminates degradation of generalization error due to overtraining resulting from the training set size, number of training epochs, and data set structure. Overall results obtained with the PSO strategy reveal the importance of optimizing parameters and weights using a consistent objective function. In fact, the parameters found using this strategy vary significantly according to, e.g., training set size and data set structure, and always differ considerably from the popular choice of parameters that allows to minimize resources.

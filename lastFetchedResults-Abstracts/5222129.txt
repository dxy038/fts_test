In this paper a speaker independent training method is presented for continuous voice to facial animation systems. An audiovisual database with multiple voices and only one speaker´s video information was created using dynamic time warping. The video information is aligned to more speakers´ voice. The fit is measured with subjective and objective tests. Suitability of implementations on mobile devices is discussed.

These days, both the cloud computing paradigm and MapReduce programming framework have become key enablers for running big data analytics and large-scale compute- and data-intensive applications. Achieving proper elasticity for cloud MapReduce jobs is a critical research problem that has been overlooked. In this paper, we focus on how to achieve proper elasticity for MapReduce jobs when executed on cloud clusters. In particular, we present an analytical queueing model that can be used to determine at any given time and under different workload conditions the minimal number of mappers and reducers needed to satisfy the Service Level Objective (SLO) response time.

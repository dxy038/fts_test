Mouse dynamics is the process of identifying individual users based on their mouse operating behaviors. Many classification algorithms have been proposed for checking usersÂ´ identity, thus it is natural to ask how well each classifier performs and how various classifiers compare to each other (e.g., to identify promising research directions). Unfortunately, we cannot conduct a valid comparison of classifiers based on the results in the literature because of inconsistent datasets, experimental conditions and methodologies across studies. The objective of this study is to obtain a dataset, to develop an objective and repeatable evaluation procedure, and to measure the performance of a range of classifiers on an equal basis. Using data from 26 subjects conducting a fixed mouse-operation task 300 times each, we implemented and evaluated twelve classifiers from the mouse-dynamics and pattern-recognition literature for a user identification task. The four top-performing classifiers achieve false-negative identification rates between 17.31% and 23.72%. The results, along with the evaluation methodology, constitute a benchmark for comparing classifiers and measuring progress for the user identification problem.

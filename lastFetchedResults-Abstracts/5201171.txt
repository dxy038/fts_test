The objective of this paper is to present and compare performances of several time-frequency analysis algorithms that characterize and associate ionosphere scintillation events focusing on carrier phase measurements obtained from three relatively close spaced antennas in Alaska. The results will be used to derive plasma drift velocity and estimate the 2-dimension size of the scintillation patches. There are two challenges in detecting and associating scintillations among closely spaced receivers. First, we have to precisely estimate the start time and end time of the scintillation event captured by each receiver. Second, we have to distinguish the fluctuation caused by multipath and receiver clock from the effects of scintillation. Our approach is to analyze the time-frequency relations of GPS L1 signal observables among the receivers. Based on past experience, scintillation causes high frequency fluctuations in the range of 0.1 to 10 Hz, while multipath only introduce an extra frequency less than 1 Hz and are typically occurring at low elevations. Receiver clock errors are eliminated using differenced measurements from L1 and L2, or from a reference satellite without scintillation. Several algorithms have been studied to perform the time-frequency spectrum analysis of carrier phases: windowed Fourier transform (WFT), Morlet Wavelet transform (MWT), Hilbert-Huang transform (HHT), and an adaptive periodogram technique (APT). All four algorithms are evaluated using both simulated and real scintillation signals. Among them, APT provides the most optimal performance in terms of precision in detecting the occurrences of scintillation events in both time-domain and frequency-domain. However, APT also has a high computational cost. Morlet Wavelet transform offers complimentary performance with significantly lower computational cost. Thus, it is implemented as the level one scintillation filter before the events are finally analyzed by APT.

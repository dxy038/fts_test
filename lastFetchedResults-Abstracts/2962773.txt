A class of unconventional neural optimization algorithms called orthonormal strongly-constrained (SOC) is presented. Here the general problem of the iterative search of maxima or minima of objective functions under the constraint of orthonormality is dealt with . After that general properties of the SOC algorithms are stated, examples are discussed relative to the cases of gradient-based and non-gradient-based learning rules. Finally, known algorithms found in literature are shown to belong to the SOC class

Suppose a vector of observations y = Hx+n stems from independent inputs x and n, both of which are Gaussian Mixture (GM) distributed, and that H is a fixed and known matrix. This work focuses on the design of a precoding matrix, F, such that the model modifies to z = HFx + n. The goal is to design F such that the mean square error (MSE) when estimating x from z is smaller than when estimating x from y. We do this under the restriction E[(Fx)<sup>T</sup>Fx] &#8804; P<sub>T</sub>, that is, the precoder cannot exceed an average power constraint. Although the minimum mean square error (MMSE) estimator, for any fixed F, has a closed form, the MMSE does not under these settings. This complicates the design of F. We investigate the effect of two different precoders, when used in conjunction with the MMSE estimator. The first is the linear MMSE (LMMSE) precoder. This precoder will be mismatched to the MMSE estimator, unless x and n are purely Gaussian variates. We find that it may provide MMSE gains in some setting, but be harmful in others. Because the LMMSE precoder is particularly simple to obtain, it should nevertheless be considered. The second precoder we investigate, is derived as the solution to a stochastic optimization problem, where the objective is to minimize the MMSE. As such, this precoder is matched to the MMSE estimator. It is derived using the KieferWolfowitz algorithm, which moves iteratively from an initially chosen F<sub>0</sub> to a local minimizer F*. Simulations indicate that the resulting precoder has promising performance.

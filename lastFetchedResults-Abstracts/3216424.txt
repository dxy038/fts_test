The representative samples can be pictured as the skeleton of a point cloud. We learn a discrete distribution defined over all samples, so that these skeleton points have large probabilities and the outliers have probabilities close to zero. The basic assumption is that any observation is generated from a nearby skeleton point. The learning objective is to minimize the communication cost from a random sample to its generation source. Experiments show that the learned distribution highlights a compact size of key positions. It is further applied to a denoising task as an indirect method of evaluation. The clustering structures of image datasets are best preserved among several methods investigated.

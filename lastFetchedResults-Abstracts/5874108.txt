This paper studies the decentralized solution of a multi-agent sparse regression problem in the form of a globally coupled objective function with a non-smooth sparsity promoting constraint. In particular, we propose a distributed primal-dual perturbation (PDP) method which combines the average consensus technique and the primaldual perturbed subgradient method. Compared to the conventional primal-dual (PD) subgradient method without perturbation, the PDP subgradient method exhibits a faster convergence behavior. In order to handle the non-smooth constraints, we propose a novel proximal gradient type perturbation point. The proposed distributed optimization algorithm can be implemented as a fully decentralized protocol, with each agent using its local information and exchanging messages between neighbors only. We show that the proposed method converges to the global optimum of the considered problem under standard convex problem and network assumptions.

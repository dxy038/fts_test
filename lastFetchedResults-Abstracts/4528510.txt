The problem of maximizing a general objective function over the hypercube {-1, 1}<sup>n</sup> is formulated as that of maximizing a multilinear polynominal over {-1, 1}<sup>n</sup>. Two methods are given for updating the state vector of the neural network, i.e., the asynchronous and the synchronous rules. They are natural generalizations of the corresponding rules for Hopfield networks with a quadratic objective function. It is shown that the asynchronous updating rule converges to a local maximum of the objective function within a finite number of time steps. A modified synchronous updating rule is presented. It incorporates both temporal as well as spatial correlations among the neurons. For the modified updating rule, it is shown that, after a finite number of time steps, the network state vector goes into a limit cycle of length <e1>m</e1>, where <e1>m</e1> is the degree of the objective function

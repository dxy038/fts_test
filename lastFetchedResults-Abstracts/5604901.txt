Benign Pro Static Hyperplasia (BPH) is estimated to effect 50% of men by the age of 50, and 75% by the age of 80. Predicting a clinical outcome prior to minimally invasive treatments for BPH would be very useful, but has not been reliable in spite of multiple assessment parameters such as symptom indices and flow rates. I our prior work we have shown the effect of greater impact feature selection has on prediction of the BPH clinical outcomes. In this work we take an in depth look at how changes to the Artificial Intelligence and Machine Learning methods can have an affect on how well the process does at predicting the outcome of the patients in the testing group. The affect of which classifier is used, to predict BPH surgical outcomes, is investigated to see if certain classifiers perform better with the data. The affect of which metric is selected for analyzing the performance of the classifier prediction is also observed. The affect of which features and how many are selected to train and predict the data is observed. Finally, the affect of using the original, unchanged, date versus a discretized version of the data is also observed. The objective in this paper is to determine, in this case, which of the above-mentioned factors affect the outcome of the predictive models, to allow the best factor selection in each case so that the best predictive method of NPH for this data, can be determined. In particular, the data is analyzed to determine if some of these factors have a larger effect on the outcome than others. Through experimental results we show which and how some factors are found to have no real influence on clinical outcome prediction, and show how in some other cases there are a few equally good choices. Here four machine learning algorithms, namely Decision Tree, Nai&#776;ve Bayes, LDA, and ADABoost are selected and used in the comparison. For prediction performance metrics comparison we use the Area Under the Curve (AUC), Accuracy (ACC), and the Ma- thew Correlation Coefficient (MCC). Both internal cross-validation and external validation are used to analyze the performance and results of the predictive models considered.

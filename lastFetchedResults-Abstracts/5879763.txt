Support vector machines (SVMs) are currently a very active research area for machine learning, data mining, etc. Sparsity control is an issue deserving further attention for the improvement of existing support vector machines techniques. This work presents two new sparsity control methods for 1- norm support vector classification. The first scheme, called SVC-sc1, is formulated by adding a penalty term in the objective function, whereas the second scheme, called SVC-sc2, is obtained by adding an extra inequality to the original optimization problem. The common goal is to reduce the number of retained support vectors. Besides mathematical formulation, we present test results on the benchmark Ripley data set. The experimental results indicate that both schemes outperform the conventional SVC, whereas SVC-sc2 has a still better performance than SVC-sc1.

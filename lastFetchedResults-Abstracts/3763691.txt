This paper considers the problem of Neyman-Pearson distributed detection. In distributed detection structures, a number of subordinate decision makers decide upon the active hypothesis based on their own data, and then transmit these decisions to one or more primary decision makers. Then the Neyman-Pearson performance criterion is deployed, the objective is to maximize the probability of detection (also known as power probability) induced by the primary decision makers, subject to a given false alarm constraint. In this formulation, the overall optimization problem reduces to the problem of threshold evaluation. This paper deals exactly with this issue. An on-line threshold learning algorithm is proposed that operates directly an data and requires-no explicit knowledge of the underlying probability distributions. The algorithm adapts recursively the pertinent threshold parameters in a way that minimizes the Kullback-Leibler distance between the observed and the desired output distribution. A formal convergence study is carried out and shows that, under some general conditions, the algorithm is strongly consistent; that is, the sequences of the produced threshold estimates converge to the optimal threshold values with probability 1. The rate of convergence is examined, and methods for controlling it are proposed. Simulation results are included and provide additional support to the theoretical arguments

The problem of creating artifact-free upscaled images appearing sharp and natural to the human observer is probably more interesting and less trivial than it may appear. The solution to the problem, often referred to also as &#8220;single-image super-resolution,&#8221; is related both to the statistical relationship between low-resolution and high-resolution image sampling and to the human perception of image quality. In many practical applications, simple linear or cubic interpolation algorithms are applied for this task, but the results obtained are not really satisfactory, being affected by relevant artifacts like blurring and jaggies. Several methods have been proposed to obtain better results, involving simple heuristics, edge modeling, or statistical learning. The most powerful ones, however, present a high computational complexity and are not suitable for real-time applications, while fast methods, even if edge adaptive, are not able to provide artifacts-free images. In this paper, we describe a new upscaling method (iterative curvature-based interpolation) based on a two-step grid filling and an iterative correction of the interpolated pixels obtained by minimizing an objective function depending on the second-order directional derivatives of the image intensity. We show that the constraints used to derive the function are related with those applied in another well-known interpolation method, providing good results but computationally heavy (i.e., new edge-directed interpolation (NEDI). The high quality of the images enlarged with the new method is demonstrated with objective and subjective tests, while the computation time is reduced of one to two orders of magnitude with respect to NEDI so that we were able, using a graphics processing unit implementation based on the nVidia Compute Unified Device Architecture technology, to obtain real-time performances.

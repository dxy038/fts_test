It is well known that software systems suffer from performance degradation due to resource shrinking and this phenomenon is referred to as Software Aging. Long running software systems tend to show degradation in performance due to exhaustion of operating systems resources, data corruption and numerical error accumulation. The primary objective of the paper is to establish the aging trend in the server virtualized system. It establishes the aging trend by showing that the average response time decreases while total available physical memory decreases. Linear regression model has been used to study the aging trend.

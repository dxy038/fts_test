We propose a topic graph based transfer learning method based on Non-negative Matrix Factorization (NMF) with generalized Kullback-Leibler (KL) divergence. Based on the Frobenius norm based NMF, a transfer learning method was proposed based on the similarity of feature spaces. We extend the previous method by utilizing generalized KL divergence based NMF so that better probabilistic interpretation can be obtained with the divergence. The proposed method is formalized as the minimization of an objective function under the divergence, and an auxiliary function for the objective function is defined. From the auxiliary function, we derive a learning algorithm with multiplicative update rules, which are guaranteed to converge. The proposed method is evaluated in terms of document clustering over several well-known benchmark datasets. Extensive experiments have been conducted on the datasets, and comparison with other transfer learning methods as well as state-of-the-art NMF methods is reported.

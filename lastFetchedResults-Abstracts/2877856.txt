Multi-task optimization is common in machine learning, filtering, communication and network problems. We focus on the nonconvex separable problem where the objective is the sum of N individual utility functions subject to a total budget constraint. By leveraging the Lagrangian dual decomposition, the dual ascent method naturally applies and can be implemented distributively. For stochastic versions of multi-task problems, we propose a simulation-based dual ascent algorithm. According to a classical result from convex geometry, the average-per-task duality gap between the primal and dual problems is bounded by O (1/N). This suggests that the nonconvex multi-task problem is getting &#8220;convexified&#8221; as the number of tasks increases. As a result, the proposed distributed dual algorithm recovers the optimal solution of the nonconvex problem with very small error.

Current research has identified the need to equip robots with perceptual capabilities that not only recognise objective entities such as visual or auditory objects but that are also capable of assessing the affective evaluations of the human communication partner in order to render the communication situation more natural and social. In equivalence to Watzlawick\Â´s statement that "one cannot not communicate" (1968) it has been found that also in human-robot interactions one cannot be not emotional. It is therefore crucial for a robot to understand these affective signals of its communication partner and react towards them. However, up to now, online emotion recognition in realtime, interactive systems has scarcely been attempted as apparently demands concerning robustness and time constraints are very high. In this paper we present an empathic anthropomorphic robot (torso) that mirrors the emotions happiness, fear and neutral as recognised from the speech signal by facial expressions. The recognition component as well as the development of the facial expression generation are described in detail. We report on results from experiments with humans interacting with the empathic robot

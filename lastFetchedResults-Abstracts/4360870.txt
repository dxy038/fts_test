Our artificial neural network (ANN) software requires nine parameters to be initialized when running an experiment. Tuning each parameter for optimum ANN performance, one at a time, is very time-consuming since a user must adjust each or a combination of these parameters to obtain optimal results. The objective of this work was to develop a program that automatically optimizes all of these parameters without user supervision. The problem was approached using a &#8220;divide and conquer&#8221; technique. The ANN results obtained with the new automated network were compared with results obtained previously with the manual method. In addition, a new stopping criterion where the network monitors its own performance to choose when to stop training was introduced. The accuracy of the new ANN was similar to the previously manually-optimized networks. The network parametersÂ´ sensitivity curves, in determining the highest correct classification rate (best accuracy), show that the momentum, learning rate, learning rate increment, and the error ratio were the most sensitive parameters; the weight-decay constant and the learning rate decrement were least sensitive on network performance. The improvements in the experimental approach allow our future experiments to be run around the clock on several computers simultaneously, and without user supervision

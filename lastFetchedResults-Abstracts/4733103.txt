Studies the problem of minimizing an objective function over the discrete set {0,1}<sup>n</sup>. It is shown that one can assume without loss of generality that the objective function is a multilinear polynomial. A gradient type neural network is proposed to perform the optimization. A novel feature of the network is the introduction of a bias vector. The network is operated in the high-gain region of the sigmoidal nonlinearities. The following comprehensive theorem is proved: For all sufficiently small bias vectors except those belonging to a set of measure zero, for all sufficiently large sigmoidal gain, for all initial conditions except those belonging to a set of measure zero, the state of the network converges to a local minimum of the objective function. This is a considerable generalization of earlier results for quadratic objective functions

At each instant of time we are required to sample a fixed number <img alt="m \\geq 1" src="/images/tex/3254.gif"/> out of <img alt="N" src="/images/tex/88.gif"/> Markov chains whose stationary transition probability matrices belong to a family suitably parameterized by a real number <img alt="\\theta" src="/images/tex/1471.gif"/> . The objective is to maximize the long run expected value of the samples. The learning loss of a sampling scheme corresponding to a parameters configuration <img alt="C = (\\theta_{1}, ..., \\theta_{N})" src="/images/tex/3255.gif"/> is quantified by the regret <img alt="R_{n}(C)" src="/images/tex/3256.gif"/> . This is the difference between the maximum expected reward that could be achieved if <img alt="C" src="/images/tex/27.gif"/> were known and the expected reward actually achieved. We provide a lower bound for the regret associated with any uniformly good scheme, and construct a sampling scheme which attains the lower bound for every <img alt="C" src="/images/tex/27.gif"/> . The lower bound is given explicitly in terms of the Kullback-Liebler number between pairs of transition probabilities.

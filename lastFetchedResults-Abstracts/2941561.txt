Multilayer feedforward neural networks with sigmoidal activation functions have been termed &#8220;universal function approximators&#8221;. Although these types of networks can approximate any continuous function to a desired degree of accuracy, this approximation may require an inordinate number of hidden nodes and is only accurate over a finite interval. These short comings are due to the standard multilayer perceptronÂ´s (MLP) architecture not being well suited to unbounded non-linear function approximation. A new architecture incorporating a logarithmic hidden layer proves to be superior to the standard MLP for unbounded non-linear function approximation. This architecture uses a percentage error objective function and a gradient descent training algorithm. Non-linear function approximation examples are used to show the increased accuracy of this new architecture over both the standard MLP and the logarithmically transformed MLP

Frequently in test, measurement and development, there is a requirement to compare two or more supposedly identical data sets. These data sets may be acquired as: test data (does the data set fall within a given level of conformance); validation data (do the results confirm the applicability of a test methodology or design process); and calibration data (the data is required to act as a benchmark against which other measurements will be assessed). An example of such data is the far-field three-dimensional radiation pattern of antennas, which may be measured repeatedly on the same or different antenna test ranges. The requirement for objective, quantitative and robust methods of assessing such data, is discussed and confirmed. In addition, the constraints placed on these assessment methods, applied by the nature of the measurement process and the measurand, are highlighted and examined. Data sets that can be used to illustrate the application of these comparison techniques are presented and a preliminary assessment of them made using previously established techniques. These data sets embody a variety of subtle and specific characteristics that stem from particular known error sources. The limitations of these established assessment techniques are discussed and used to motivate the development of newer, more sophisticated analysis, where the data sets are further processed to yield objective measures of comparison. A variety of new assessment techniques that satisfy the aforementioned constraints are then presented and their various merits are compared and contrasted to illustrate their applicability to the classification and analysis of large data sets derived from near-field antenna measurements.

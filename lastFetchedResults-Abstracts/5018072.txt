PlattÂ´s sequential minimal optimization has been widely adopted in modern implementations of support vector machines. This work points out only caching the gradients for unbounded support vectors in sequential minimal optimization affects efficiency. A better principle is to cache gradients for all vectors frequently checked. This paper also shows searching for working pairs which maximizes the gradient differences conducted more aggressively. The results on extending the search for pairs maximizing objective changes shows no extra cost of kernel evaluations, but demonstrates better convergence rate and comparable runtime.

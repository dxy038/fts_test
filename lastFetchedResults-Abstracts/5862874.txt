The importance of testing approaches that exploit error tolerance to improve yield has previously been established. Error rate, defined as the percentage of vectors for which the value at a circuitÂ´s output deviates from the corresponding error-free value, has been identified as a key metric for severity. In error-rate testing every chip that has an error rate greater than or equal to a threshold specified by the application is unacceptable for the application and discarded; all other chips are acceptable. The objective of error-rate testing is to reject every unacceptable chip while accepting all (or a maximum number) of the acceptable chips. We previously showed that it is not always possible to generate a test set that detects all unacceptable faults, i.e., faults that cause an error rate greater than or equal to the threshold error rate, without detecting some of the acceptable faults, i.e., faults that cause an error rate less than the threshold. In this paper, we introduce the new notion of multi-vector testing and prove that this notion enables us to detect all unacceptable faults without detecting any of the acceptable faults. We derive an upper bound on the size of such a test for a general case. As this universal bound can be large in some cases, we use a structural approach and find much tighter upper bounds for special classes of circuits. Experiments on benchmark circuits show that the required test-sizes for arbitrary circuits are much lower than our universal bounds, and practically useful.

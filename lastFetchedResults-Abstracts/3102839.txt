This paper presents a new variant of AdaBoost based on Viola and Jonespsila framework, called Z-AdaBoost. Instead of modifying the mechanism of selecting optimal weak classifiers as other variants do, Z-AdaBoost strengthens the discriminating power of weak classifiers themselves by expanding them into 2-thresholded ones, which guarantees a better classification with smaller error. And a linear online algorithm is adopted to select the optimal values for the 2 thresholds. The idea is simple as the bound on the accuracy of the final hypothesis improves when any of the weak hypotheses is improved. The experimental results under a rigid and objective detection criterion on MIT+CMU upright face test set show that Z-AdaBoost explores some potentially discriminative features that are ignored during the weak learning process in AdaBoost, resulting in a comparably effective detector with much fewer weak classifiers and features. The approach of Z-AdaBoost can be easily incorporated in other boosting algorithms to even improve their performance.

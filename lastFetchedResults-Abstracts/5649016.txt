A hierarchically intelligent control algorithm, accepting discrete voice input commands and feed-back visual information from a television camera has been developed to control the MIT Scheinman electric arm in real time. The project developed to assist immobilized people, confined in limited environments, employs a task-oriented hierarchically intelligent control approach. The motion of the arm, though considered to be composed of motion of the wrist and orientation of the hand, results from the simultaneous execution of these two distinct motions. A task-directed linguistic command or voice command is recognized, interpreted, and decoded into sequences of subtasks. The first sub-task corresponds to the motion of the wrist which is controlled by the suboptimal feedback controller. The remaining four subtasks are further broken down into combinations of six primitive movements which govern the position/orientation of the hand. The objective of the visual recognition algorithm is to identify objects and their locations surrounding the arm from its environmental library. The library is then updated to initiate the arm to complete its execution of task. The real-time implementation of the algorithm on the MIT arm of the Advanced Automation Research Laboratory (AARL) of Purdue University has demonstrated the feasibility of the algorithm and its advantages in terms of time execution and efficiency of implementation.

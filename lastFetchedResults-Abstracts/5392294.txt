There has been significant recent interest in sparse metric learning (SML) in which we simultaneously learn both a good distance metric and a low-dimensional representation. Unfortunately, the performance of existing sparse metric learning approaches is usually limited because the authors assumed certain problem relaxations or they target the SML objective indirectly. In this paper, we propose a generalized sparse metric learning method (GSML). This novel framework offers a unified view for understanding many of the popular sparse metric learning algorithms including the sparse metric learning framework proposed, the large margin nearest neighbor (LMNN), and the D-ranking vector machine (D-ranking VM). Moreover, GSML also establishes a close relationship with the pairwise support vector machine. Furthermore, the proposed framework is capable of extending many current non-sparse metric learning models such as relevant vector machine (RCA) and a state-of-the-art method proposed into their sparse versions. We present the detailed framework, provide theoretical justifications, build various connections with other models, and propose a practical iterative optimization method, making the framework both theoretically important and practically scalable for medium or large datasets. A series of experiments show that the proposed approach can outperform previous methods in terms of both test accuracy and dimension reduction, on six real-world benchmark datasets.

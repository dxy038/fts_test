Convex optimization problems involving information measures have been extensively investigated in source and channel coding. These measures can also be successfully used in inverse problems encountered in signal and image processing. The related optimization problems are often challenging due to their large size. In this paper, we derive closed-form expressions of the proximity operators of Kullback-Leibler and Jeffreys-Kullback divergences. Building upon these results, we develop an efficient primal-dual proximal approach. This allows us to address a wide range of convex optimization problems whose objective function expression includes one of these divergences. An image registration application serves as an example for illustrating the good performance of the proposed method.

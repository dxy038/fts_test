The primary objective of this paper is to demonstrate that the adaptive reinforcement learning method proposed by Asada-Izumi (1989) is efficient and useful in learning a compliance control law in a class of robotic assembly tasks. A simple ball-aligning task is used as an example, where a robot is required to move a ball to the corner of a box. In the simulation, the robot is initially provided with only position feedback gains to follow the nominal trajectory. However, at each attempt, the location of the box is randomly deviated from the nominal position as uncertainty of the environment and, therefore, compliant motion control is required to guide the ball to the corner of the box. After repeating the collision with the walls of the box, the robot can successfully generate force feedback gains to modify its nominal motion. Our results show that the new learning method can be used to learn a compliance control law effectively.

This paper presents a new distributed optimization technique, the inexact fast alternating minimization algorithm (inexact FAMA), that allows for inexact local computation as well as for errors resulting from limited communication. We show that inexact FAMA is equivalent to the inexact accelerated proximal-gradient method applied to the dual problem and derive an upper-bound on the number of iterations for convergence for inexact FAMA. The second contribution of this work is that a weakened assumption for FAMA, as well as for its inexact version, is presented. The new assumption allows the strongly convex objective in the optimization problem to be subject to convex constraints, while still guaranteeing convergence of the algorithm, which facilitates its application to control problems. We apply inexact FAMA to distributed MPC problems and derive the convergence properties of the algorithm for this special case. By employing the upper-bound on the number of iterations, sufficient conditions on the errors are provided, which ensure converge of the algorithm. Finally, we demonstrate the performance of the algorithm and the theoretical findings using a randomly generated distributed MPC example.

The objective of the research is to develop a new method of human-machine interaction that reflects and harnesses the multimodal expressive abilities and potentials of people with severe speech and motor impairment due to cerebral palsy. Human-human interaction within the framework of drama and mime was used to elicit gestural behavior suitable for human-machine interaction and to explore human factors issues. Subjects classified as having little or no functional use of their extremities were able to produce a rich repertoire of gestures and mimes involving the arms, head, face, legs, torso, and vocalizations. A gesture subset comprising dynamic arm gestures have been monitored using biomechanical and bioelectric sensors. Neural computing, pattern recognition and multi-sensor fusion techniques are being used to analyze the gestures and to develop a gesture recognition system. Time delay neural networks were able to recognize 14 dynamic arm gestures from a 17 year old male classified as having no functional use of his arms

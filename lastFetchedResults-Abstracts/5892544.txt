Summary form only given, as follows. The author explores the concept of influence functions and their application in backpropagation learning. One reason for backpropagation´s popularity is the fact that it is capable of fitting functions involving high-order interactions. Most real problems not only require detecting nonlinear interactions, but in many cases involve fitting relationships which are contaminated by diverse noise conditions. Typically, backpropagation learning is achieved by minimizing the sum of squared errors. It is well known that least-squares estimators are maximally efficient when the noise is Gaussian but that their performance rapidly deteriorates under non-Gaussian noise. The author´s objective is to make backpropagation resistant to various types of noise. This approach uses objective functions which, in other contexts, are known to be more robust than least squares. Simulations show that backpropagation with robust objective functions indeed learns well and generates solutions that are resistant to a wide variety of noise distributions.&lt;<etx>&gt;</etx>

Typical digital cameras use a single-chip image sensor covered with a mosaic of red, green, and blue color filters for capturing color information. At each pixel location, only one of the three color values is known. The interpolation of the two missing color values at each pixel in a color filter array image (CFA) is called demosaicing. In this paper, we propose a novel training-based approach for computing the missing green pixels in a CFA. The algorithm works by extracting a multi-dimensional feature vector comprising derivatives of various orders computed in a spatial neighborhood of the pixel being interpolated. Using a statistical machine learning framework, the feature vector is then used to predict the optimal interpolation direction for estimating the missing green pixel. The parameters of the statistical model are learned in an offline training procedure using example training images. Once the green channel has been estimated, the red and blue pixels are estimated using bilinear interpolation of the difference (chrominance) channels. Both subjective and objective evaluations show that the proposed demosaic algorithm yields a high output image quality. The algorithm is computationally and memory efficient, and its sequential architecture makes it easy to implement in an imaging system.

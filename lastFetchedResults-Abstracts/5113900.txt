Natural interaction between humanoid robots and humans is one of the major goals of the HRI field. Two major requirements for advancing this direction of research are the availability of human-human and base human-robot interaction datasets for training and evaluation purposes and the availability of general agreed upon objective metrics for judging the performance of proposed robots and algorithms. In this paper we report details of the H<sup>3</sup>R explanation corpus dataset of human-human and base human-robot interactions in assembly/disassembly explanation scenarios that combines five kinds of data: video, audio, motion tracking, subjective, and physiological data. 44 subjects and 66 sessions were conducted during this experiment. The corpus contains 22 natural human-human interactions, 22 un-natural human-human interactions, and 22 baseline human-robot interactions. To our best knowledge this is the first database that combines these five data types and three types of interactions. The paper also reports the first usage of this explanation corpus to compare subjective and physiological evaluations of various dimensions of listenerpsilas behavior.

In transfer learning scenarios, previous discriminative dimensionality reduction methods tend to perform poorly owing to the difference between source and target distributions. In such cases, it is unsuitable to only consider discrimination in the low-dimensional source latent space since this would generalize badly to target domains. In this paper, we propose a new dimensionality reduction method for transfer learning scenarios, which is called transferable discriminative dimensionality reduction (TDDR). By resolving an objective function that encourages the separation of the domain-merged data and penalizes the distance between source and target distributions, we can find a low-dimensional latent space which guarantees not only the discrimination of projected samples, but also the transferability to enable later classification or regression models constructed in the source domain to generalize well to the target domain. In the experiments, we firstly analyze the perspective of transfer learning in brain-computer interface (BCI) research and then test TDDR on two real datasets from BCI applications. The experimental results show that the TDDR method can learn a low-dimensional latent feature space where the source models can perform well in the target domain.

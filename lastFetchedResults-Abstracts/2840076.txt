Metadata research for music digital libraries has traditionally focused on genre. Despite its potential for improving the ability of users to better search and browse music collections, music subject metadata is an unexplored area. The objective of this study is to expand the scope of music metadata research, in particular, by exploring music subject classification based on user interpretations of music. Furthermore, we compare this previously unexplored form of user data to lyrics at subject prediction tasks. In our experiment, we use datasets consisting of 900 songs annotated with user interpretations. To determine the significance of performance differences between the two sources, we applied FriedmanÂ´s ANOVA test on the classification accuracies. The results show that user-generated interpretations are significantly more useful than lyrics as classification features (p &lt;; 0.05). The findings support the possibility of exploiting various existing sources for subject metadata enrichment in music digital libraries.

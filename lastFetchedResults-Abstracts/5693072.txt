The same word uttered by different people have different waveforms, similarly the same word uttered by the same person at different time will also have some degree of variations in waveforms. In this experiment a set of ten words was selected and each word was uttered ten times by ten different speakers. The objective of this work is to extract the information conveyed in the signal, especially if the information is not directly recognizable as in the case of the speech signals. One powerful tool is the spectral analysis, where important properties of a signal are often very clearly depicted in the frequency domain, there are always situations where the spectral content of a signal is changing in time. In these situation, joint time-frequency methods, such as the spectrogram representations are used. The spectrogram image representation is then used as an input image to train a pulse coupled neural network (PCNN) which outputs a feature time series representing the number of neurons that fired each time the same image was presented. The usable length of this time-series feature (icon) is about 50 data points which is very small as compared to 512&#215;512 data points to represent an image. The features are found to be very robust and show high promise for speech and speaker recognition

It is shown that from the viewpoint of optimization theory a proper form of the Tank-Hopfield network for linear programming may be considered as a means to fulfil the Kuhn-Tucker optimality conditions. Due to the nature of the network, however, the convergence state is not the exact solution but an approximation. To get a better approximate solution, a new network formulation is introduced. The convergence state can be made very close to the exact solution by sufficiently increasing the network parameter &#955;. The result of this work is not limited to linear programming, it can be applied directly to any nonlinear programming problem whose objective function and constraints are convex and differentiable. This work may also be used as a foundation for the application of artificial neural networks to more general optimization problems

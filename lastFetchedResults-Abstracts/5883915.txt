A fundamental question in the field of communication-limited control is how low the closed loop data rate can be made before a given dynamical system is impossible to stabilise by any coding and control law. Analogous to the role of entropy in Shannon source coding, this number defines the smallest data rate sufficient to achieve ´reliable´ closed loop performance. The objective here is to analyse this quantity for a general, finite-dimensional, discrete-time stochastic linear plant. By inductive arguments employing the entropy power inequality of information theory, an explicit expression for the infimum mean-square-stabilising data rate is derived, under very mild conditions on the initial state and noise probability densities.

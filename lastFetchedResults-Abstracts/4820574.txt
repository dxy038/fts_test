An efficient stochastic method for continuous optimization problems is presented. Combining a novel global search with typical local optimization methods, the proposed method specializes in hard optimization problems such as minimizing multimodal or ill-conditioned unimodal objective functions. Extensive numerical studies show that, starting from a random initial point, the proposed method is always to find the global optimal solution. Computational results in comparison with other global optimization algorithms clearly illustrate the efficiency and accuracy of the method. As traditional supervised neural-network training is formulated as a continuous optimization problem, the method presented can be applied to neural-network learning.

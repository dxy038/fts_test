In this paper, the performance of the ALOHA and CSMA MAC protocols are analyzed in spatially distributed wireless networks. The main system objective is correct reception of packets, and thus the analysis is performed in terms of outage probability. In our network model, packets belonging to specific transmitters arrive randomly in space and time according to a 3-D Poisson point process, and are then transmitted to their intended destinations using a fully-distributed MAC protocol. A packet transmission is considered successful if the received SINR is above a predefined threshold for the duration of the packet. Accurate bounds on the outage probabilities are derived as a function of the transmitter density, the number of backoffs and retransmissions, and in the case of CSMA, also the sensing threshold. The analytical expressions are validated with simulation results. For continuous-time transmissions, CSMA with receiver sensing (which involves adding a feedback channel to the conventional CSMA protocol) is shown to yield the best performance. Moreover, the sensing threshold of CSMA is optimized. It is shown that introducing sensing for lower densities (i.e., in sparse networks) is not beneficial, while for higher densities (i.e., in dense networks), using an optimized sensing threshold provides significant gain.

Self-adaptive mutations are known to endow evolutionary algorithms (EA) with the ability of locating local optima quickly and accurately, whereas it was unknown whether these local optima are finally global optima provided that the EA runs long enough. In order to answer this question, it is assumed that the (1+1)-EA with self-adaptation is located in the vicinity P of a local solution with objective function value &#949;. In order to exhibit convergence to the global optimum with probability one, the EA must generate an offspring that is an element of the lower level set S containing all solutions (including a global one) with objective function value less than &#949;. In case of multimodal objective functions, these sets P and S are generally not adjacent, i.e., min{||x-y||:x&#8712;P, y&#8712;S}&gt;0, so that the EA has to surmount the barrier of solutions with objective function values larger than &#949; by a lucky mutation. It will be proven that the probability of this event is less than one even under an infinite time horizon. This result implies that the EA can get stuck at a nonglobal optimum with positive probability. Some ideas of how to avoid this problem are discussed as well

The authors propose a new family of multi-layer, feed-forward network (FFN) architectures. This framework allows examination of several feed-forward networks, including the well-known multi-layer perceptron (MLP) network, the likelihood network (LNET) and the distance network (DNET), in a unified manner. They then introduce a novel formulation which embeds network parameters into a functional form of the classifier design objective so that the networkÂ´s parameters can be adjusted by gradient search algorithms, such as the generalized probabilistic descent (GPD) method. They evaluate several discriminative three-layer networks by performing a pattern classification task. They demonstrate that the performance of a network can be significantly improved when discriminative formulations are incorporated into the design of the pattern classification networks

Consider that data gathered from sensors are lossy encoded at low bit rate in multihop wireless sensor networks. Particularly, each non-terminal sensor node is instructed to jointly encode its sensing data and the correlated data received from its nearby nodes by exploiting spatial correlation. In the paper, the optimization of rate-distortion allocation is addressed. The objective is to find an optimal rate-distortion allocation at sensors within the given target distortion such that the network communication cost is minimized. An analytical optimal allocation is presented, whereby a fully distributed allocation scheme is designed. The proposed methods are evaluated via extensive simulations using real-world data traces. The results indicate that the optimal allocation strategy improves the uniform allocation method at least by a factor of 9.7 on average. Furthermore, this improvement would be more significant while the sensorsÂ´ locations are non-uniform or the data patterns gathered from sensors are changed greatly. In summary, the results indicate that the total network communication cost can be significantly reduced by optimally exploiting the rate-distortion allocation among the resource-constrained sensors.

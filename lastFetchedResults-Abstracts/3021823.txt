This paper addresses the problem of connecting words to image objects for efficient image retrieval. Our purpose is to bridge the gap between the user´s high-level retrieval semantics and the results obtained from objective models using low-level features. The proposed method uses a novel multi-feature-based diffusion framework to obtain a region-based visual image representation. We focus on exploring the results of psychophysical studies, and propose an assignment of low-level visual features to related adjectives and nouns and connect them to objects by using perceptual clustering. We also determine concepts and categories to create semantic relations. Evaluation of 15,000 natural images shows that more accurate modeling of the user´s subjective similarity interpretation, and thus higher retrieval accuracy was achieved by using an additional layer of words representing lower level semantics and also by using various searching modes and options supported by dynamically generated index structures.

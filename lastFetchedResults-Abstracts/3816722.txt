To achieve good generalization in supervised learning, the training and testing examples are usually required to be drawn from the same source distribution. In this paper, we propose a method to relax this requirement in the context of logistic regression. Assuming <i>D</i> <sup>p</sup> and <i>D</i> <sup>a</sup> are two sets of examples drawn from two different distributions <i>T</i> and <i>A</i> (called concepts, borrowing a term from psychology), where <i>D</i> <sup>a</sup> are fully labeled and <i>D</i> <sup>p</sup> partially labeled, our objective is to complete the labels of <i>D</i> <sup>p</sup>. We introduce an auxiliary variable mu for each example in <i>D</i> <sup>a</sup> to reflect its mismatch with <i>D</i> <sup>p</sup>. Under an appropriate constraint the mus are estimated as a byproduct, along with the classifier. We also present an active learning approach for selecting the labeled examples in <i>D</i> <sup>p</sup>. The proposed algorithm, called <i>migratory</i> <i>logistic</i> <i>regression</i>, is demonstrated successfully on simulated data as well as on real measured data of interest for unexploded ordnance cleanup.

The source of input data for soil physical properties may contribute to uncertainty in simulated catchment response. The objective of this study was to quantify the uncertainty in catchment surface runoff and erosion predicted by the physically based model LISEM, as influenced by uncertainty in soil texture and SOM content, and the pedotransfer function derived soil water retention curve, hydraulic conductivity, aggregate stability and cohesion. LISEM was first calibrated using measured data in a sub-catchment, and then run for the whole catchment for a summer storm event with basic input data from two data sources: soil series specific generic data from the national soil survey database, and measured data collected in a grid within the catchment. The measured data were assigned in two ways: mean values per map unit, or random distribution (50 realizations) per map unit. The model was run both for a low risk situation (crop covered surface) and a high risk situation (without crop cover and with reduced aggregate stability and cohesion). The main results were that 1) using non-local database data yielded much higher peak discharge and five to six times higher soil loss than using locally measured data, 2) there was little difference in simulated runoff and soil loss between the two approaches (mean value versus random distribution) to assign locally measured data, 3) differences between the 50 random realizations were insignificant, for both low-risk and high-risk situations, and 4) uncertainty related to input data could result in larger differences between runs with different input data source than between runs with the same input data source but extreme differences in erosion risk. The main conclusion was that inadequate choice of input data source can significantly affect general soil loss and the effect of measures.

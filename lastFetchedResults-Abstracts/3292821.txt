Class imbalance (CI) is common in most non synthetic datasets, which presents a major challenge for many classification algorithms geared towards optimized overall accuracy whenever the minority class risk loss is often higher than the majority class one. Support vector machine (SVM), a machine learning (ML) technique deeply rooted in statistics, maximizes linear margins between classes and generalizes well on yet to be seen data as long as the dataset is not severely imbalanced. Motivated to improve classification of imbalanced datasets using SVM standard formulation, we propose in this study a novel minority SVM (MinSVM) that achieves, with the addition of one constraint to the SVM objective function, separating boundaries that are closer to the majority class. Consequently, the minority data points are favored, and the probability of being misclassified becomes smaller. Experimental results support MinSVM claims and motivate to follow on research.

In this paper, we study semisupervised linear dimensionality reduction. Beyond conventional supervised methods which merely consider labeled instances, the semisupervised scheme allows to leverage abundant and ample unlabeled instances into learning so as to achieve better generalization performance. Under semisupervised settings, our objective is to learn a smooth as well as discriminative subspace and linear dimensionality reduction is thus achieved by mapping all samples into the subspace. Specifically, we present the transductive component analysis (TCA) algorithm to generate such a subspace founded on a graph-theoretic framework. Considering TCA is nonorthogonal, we further present the orthogonal transductive component analysis (OTCA) algorithm to iteratively produce a series of orthogonal basis vectors. OTCA has better discriminating power than TCA. Experiments carried out on synthetic and real-world datasets by OTCA show a clear improvement over the results of representative dimensionality reduction algorithms.

Recently, O. C. Hamsici points out that, LDAÂ´s linear approximation deviates from the original intention of minimizing the Bayes errors, and proposes the Bayes optimality based linear discriminant method. However, the cumulative distribution function employed by their method incurs numerous possible sequences when projected to the 1-dimensional subspace, and moreover, the covariance whiten scheme in the original space neglects the geometry change accompany with the upgrading feature vectors. In this paper, we propose a new algorithm which employs a similarity matrix to evaluate thediscriminative power of each class. Different from the Bayes optimality method, our algorithm introduces an appropriateexpression to optimize the Bayes error, which provides a gradient based scheme to solve the feature vectors in arbitrary reduced dimension, which avoids the 1-dimensional space projection problem, and moreover, dynamically revising mean and covariance in the objective space discloses more accurate discriminant information. The experimental results show the promise of our method.

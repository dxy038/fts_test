The authors generalize and extend previously known results on obtaining probability estimates from neural network classifiers. In particular, the authors derive necessary and sufficient conditions for an objective function which minimizes to a probability. The objective function <e1>L</e1>(<e1>x</e1>,<e1>t</e1>) was found to be uniquely specified by the function <e1>L</e1>(<e1>x</e1>,0). This function <e1>L </e1>(<e1>x</e1>,0) was found to satisfy further restrictions when a condition of logical symmetry is required. These restrictions and the relation between <e1>L</e1>(<e1>x</e1>,<e1>t</e1>) and <e1>L</e1>(<e1>x </e1>,0) define the class of all objective functions which minimize to a probability. The two simplest functions in this class were found to be the well-known mean-squared error and cross entropy objective functions

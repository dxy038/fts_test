Sparse reconstruction approaches using the re-weighted &#8467;<sub>1</sub>-penalty have been shown, both empirically and theoretically, to provide a significant improvement in recovering sparse signals in comparison to the &#8467;<sub>1</sub>-relaxation. However, numerical optimization of such penalties involves solving problems with &#8467;<sub>1</sub>-norms in the objective many times. Using the direct link of reweighted &#8467;<sub>1</sub>-penalties to the concave log-regularizer for sparsity, we derive a simple proximal-like algorithm for the log-regularized formulation. The proximal splitting step of the algorithm has a closed form solution, and we call the algorithm log-thresholding in analogy to soft thresholding for the &#8467;<sub>1</sub>-penalty. We establish convergence results, and demonstrate that log-thresholding provides more accurate sparse reconstructions compared to both soft and hard thresholding. Furthermore, the approach can be directly extended to optimization over matrices with penalty for rank (i.e. the nuclear norm penalty and its re-weighted version), where we suggest a singular-value log-thresholding approach.

We propose a nonlinear fusion model to optimally reconstruct depth maps from monocular cues. The authors in [1] showed the effectiveness of estimating depth from individual cues, which saves bandwidth without compromising the quality. In this paper, we combine such depth map estimates as a Sensor Fusion problem. These depth map estimates are extracted from color, motion and texture structure of the scene. We address the combination of monocular cues as a nonlinear optimization problem with linear constraints. 2D and 3D objective quality metrics are used as reliability metrics. At first, we perform Particle Swarm Optimization based on PSNR to obtain an initial estimate for the linear weights. Then, we use these weights to perform Active-Sets based on 3VQM. We tested our approach on various video sequences with different monocular characteristics. Experimental results show that the quality of the rendered views based on combined depth maps is comparable to the quality of rendered views based on ground truth depth maps while the savings in bandwidth is up to 38:8%.

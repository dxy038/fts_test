Visual odometry is typically formulated as a descriptor-based image feature tracking problem, followed by outlier rejection and simultaneous estimation of the scene structure and camera motion. We propose a fundamentally different formulation for the stereo case: a multi-scale search over pose to estimate the transformation that best aligns two sparse point clouds in image space. This has three main consequences. First, data association is descriptorless and implicit, supporting the use of features with indistinct appearance, such as edge features. Second, outlier rejection is subsumed by the use of a robust kernel and a joint feature alignment objective. Third, the method is robust to local minima, in contrast to coarse-to-fine or iterative approaches. This paper details the proposed method, which we call Perspective Alignment Search (PAS), integrated into an edge feature visual odometry system, and an evaluation against a LIDAR-based SLAM solution.

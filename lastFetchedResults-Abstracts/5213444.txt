Two fundamental neural network structures with feedback are considered: a structure containing a fusion center, and a Hopfield network structure. The objective of the two structures is assumed to be binary hypothesis testing. The optimal operations for the Neyman-Pearson criterion are established, for stationary and memoryless hypotheses, and the time evolution of the induced power sequences is developed. The asymptotic performance of the two structures, in terms of asymptotic relative efficiency, is studied and the effects of the feedback are quantified. In addition, the effects of robust data operations per neural element are studied as well. The benefits of feedback and the robust operations are established

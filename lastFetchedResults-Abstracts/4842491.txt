This paper discusses the Improvement of Concave-Convex procedure, where the objective function in optimization problem can be decomposed into a convex function minus a generalized differential function. While preserving the property of monotonic decreasing for optimization objective function, the convergence conditions of this procedure and the scope it can be applied to were also improved greatly. Use the properties of sub-gradient and of convex function to prove thess procedures are globally descent convergent. The optimization problem it solved can be smooth or non-smooth objective functions. Meanwhile, the global convergence of this procedure can be used for analyzing the stability of Hopfield neural networks. Also it can be used both as a new way to understand existing optimization algorithms and as a procedure for generating new algorithms.

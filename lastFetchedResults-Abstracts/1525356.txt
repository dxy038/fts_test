In the context of interpretive chromatographic optimisation, robustness is usually calculated by introducing deliberated shifts in the nominal optimal conditions and evaluating their effects on the monitored objective function, mimicking thus the experimental procedures used in method validation. However, such strategy ignores a major source of error: the uncertainties associated to the modelling step, that may give rise to deceiving results when conditions that were expected to yield baseline separation are reproduced in the chromatograph. Two approaches, based on the peak purity concept, are here proposed to evaluate the robustness of the objective function under the perspective of measurement errors and modelling. The first approach implements these uncertainties as an extra band broadening for each chromatographic peak. The second one implements them as peak fluctuations in simulated replicated assays, which gives rise to a distribution of peak purities, easily computed through Monte-Carlo simulations. Both approaches predict satisfactorily a decreased separation capability, with respect to the conventional approach, for those situations where the uncertainties in peak position make the objective function critical. The first approach is less optimistic and formally less rigorous than the second one, but its computation is simpler. It can be used to map the critical resolution regions, to be comprehensively appraised further by the slower, although more rigorous, Monte-Carlo approach.

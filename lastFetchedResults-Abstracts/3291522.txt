We develop simple queuing models for a single node in a server farm and analytically study the impact of switching delay on the performance-energy trade-off. The objective is to compare how an optimized static speed scaling scheme performs against two (gated and linear) optimized dynamic speed scaling schemes, where the processor can be switched off when it is idle but the penalty is the switching delay (time to wake up the processor from the off state). In the gated scheme, the processor speed is zero when the server is idle and constant otherwise, and in the linear scheme the processing speed scales linearly with the number of jobs. Our results demonstrate that the switching delay can have a considerable impact on the optimal trade-off. The linear scheme is always better than the gated scheme and, when the switching delays are long, even the static scheme can be better. In practice, the trade-off is affected highly by the parameters and our models allow an explicit evaluation of the trade-off.

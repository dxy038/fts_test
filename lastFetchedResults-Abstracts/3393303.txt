We consider the problem of optimal allocation of measurement resources, when: (1) the total measurement budget and time duration of measurements are fixed, and (2) the cost of an individual measurement varies inversely with the (controllable) measurement accuracy. The objective is to determine the time-distribution of measurement variances that minimizes a measure of error in estimating a discrete-time, vector stochastic process with known auto-correlation matrix using a linear estimator. The metric of estimation error is the trace of weighted sum of estimation error covariance matrices at various time indices. We show that this problem reduces to a nonlinear optimization problem with linear equality and inequality constraints. The solution to this problem is obtained via a variation of the projected Newton method. For the special case when the vector stochastic process is the state of a linear, finite-dimensional stochastic system, the problem reduces to the solution of a nonlinear optimal control problem. In this case, the gradient and Hessian with respect to the measurement costs are obtained via the solution of a two-point boundary value problem and the resulting optimization problem is solved via a variation of the projected Newton method. The proposed method is illustrated using four examples.&lt;<etx>&gt;</etx>

A new efficient method is developed for optimal maximum likelihood (ML) decoding of an arbitrary binary linear code based on data received from a Gaussian channel. The decoding algorithm is based on minimization of a difference of two monotonic objective functions subject to the 0-1 constraint of bit variables. The iterative process converges to the global optimal ML solution after a finite number of steps. The proposed algorithmÂ´s computational complexity depends on the input sequence length k which is much less than the codeword length n, especially for codes with small code rates. The viability of the developed method is verified through simulations on different coding schemes

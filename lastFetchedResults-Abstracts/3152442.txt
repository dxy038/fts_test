This paper presents a method for designing semi-supervised classifier trained on labeled and unlabeled instances. We explore the trade-off between maximizing a discriminative likelihood of labeled data and a generative likelihood of labeled and unlabeled data. Moreover, mixture models are an interesting and flexible model family. The different uses of mixture models include for example generative models and density estimation. This paper investigates semi-supervised learning of mixture models using a unified objective function taking both labeled and unlabeled data into account. We conducted experiments on the WebKB and 20NEWSGROUPS. The results show that unlabeled data results in improvement in classification accuracy over the supervised model.

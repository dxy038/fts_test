We consider a detection problem for Gaussian signal model. It is well known that likelihood-ratio test (LRT) is the optimal local sensor decision for both Bayesian and Nayman-Pearson (NP) criteria for a binary hypotheses testing problem. In general, the threshold computation of LRT is computationally intractable because of the high-dimensional integral caused by the multisensor measurements. Thus, it is very difficult to compute the detection probability and average error probability (Pe) exactly. In this case, approximation approaches are necessary in real problems. In this paper, we use relative entropy between two density functions as the measure of detection performance to study the power allocation in detection of Gaussian signals. The objective problem is not a convex problem. By the enhanced Fritz John necessary conditions we prove the local optimal solution satisfies the KKT conditions and also show existence and uniqueness of Lagrange multiplier. Then by the quality of convex function we show that D(P<sub>1</sub> &#8741; P<sub>0</sub>) is a monotonically increasing function of tr(&#931;<sub>s</sub>). Finally, we give some optimal power allocation matrices in special signal channels. Numerical examples show that average error probability (Pe) of new method we proposed and ergodic method decreases monotonically with transmit power P.

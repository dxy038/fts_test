Objective
Deferoxamine therapy in lifelong transfusion-dependent anaemias, as beta-thalassemia major, is associated with an increased risk of ototoxic changes. With increasing survival rates, prevention and/or early detection of ototoxicity are important for providing management options. The predictive value of pure-tone audiometry in early detection of ototoxicity has been questioned, particularly in the higher frequencies. Otoacoustic emissions appear to be more sensitive to cochlear insult than the conventional pure-tone audiometry. The aim of our study was to compare the efficacy of otoacoustic emissions (distortion-product otoacoustic emissions) with that of pure-tone audiometry as method of audiological monitoring.
Methods
Baseline audiometric (0.25â€“8 kHz) and otoacoustic emission testing (distortion-product otoacoustic emissions) was conducted in a group of patients with beta-thalassemia major, 60 of whom met the criteria for inclusion in the study. Comparisons were performed between baseline measurements and those recorded after 20 months. Distortion-product otoacoustic emissions were obtained as DP-grams. The DP-gram amplitude was determined for each child.
Results
Threshold changes from baseline were found to be statistically significant from 4 to 8 kHz in 68.4% of the subjects (P < 0.01). Distortion-product otoacoustic emissions demonstrated a significant threshold shift and a decreased amplitude in the frequencies >3 kHz (P < 0.05). Furthermore, DP-gram amplitude also reduced significantly at 3 kHz (P < 0.05) without any similar change in pure-tone audiometry.
Conclusions
As ototoxicity screening tool DP-gram was extremely sensitive and superior to pure-tone audiometry. Their use is recommended for regular monitoring of cochlear function, aiming in prevention of permanent damage.


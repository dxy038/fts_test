A novel objective function is presented that incorporates both local and global errors as well as model parsimony in the construction of wavelet neural networks. Two methods are presented to assist in the minimization of this objective function, especially the local error term. First, during network initialization, a locally adaptive grid is utilized to include candidate wavelet basis functions whose local support addresses the local error of the local feature set. This set can be either user-defined or determined using information derived from the wavelet transform modulus maxima representation. Next, during the network construction, a new selection procedure based on a subspace projection operator is presented to help focus the selection of wavelet basis functions to reduce the local error. Simulation results demonstrate the effectiveness of these methodologies in minimizing local and global error while maintaining model parsimony and incurring a minimal increase on computational complexity

We present an advanced and comprehensive semiconductor optical amplifier model to analyze the propagation and amplification of 10 to, in principle, 1280 Gb/s ultra-short optical pulse sequences. Through appropriate transformation, the partial differential propagation-rate equation problem is numerically solved in a two-dimensional grid of fine resolution. The corresponding simulator, entirely programmed in the graphical language LabVIEW, is compared to an identical simulator implemented in the popular high-level text-based language Matlab. Special care has been taken to implement the same set of algorithms using equivalent codes for each language, such that a fair and objective one-to-one comparison can be carried out. In terms of computational time the LabVIEW simulator shows a tenfold outperformance, compared to its text-based identical counterpart implemented in Matlab, when typical bit sequences at 40 Gb/s with a length of 8 to 1024 bits are tested. The performance results presented here apply to a broader set of device modeling scenarios.

Collaboration between distributed human and robot partners during military operations is becoming more necessary. In order to enable efficient real-time communication, it is important to develop user interfaces that support robust spoken language understanding capabilities. As a step toward achieving this objective, this work examines the role of shared gaze between a human and robot during remote spoken collaboration engaged in a distributed military operations. Preliminary results have shown that an interface that supports shared gaze between a human and robot for a remote collaborative HRI search task has potential to improve automated language understanding as well as task efficiency.

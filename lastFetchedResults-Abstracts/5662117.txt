The objective of this paper is to demonstrate the limitations of the variance-time (VT) test as a statistical tool for inferring long-range dependence (LRD) in network traffic. Since the early Bellcore studies, LRD has been in the center of a continuous debate within the teletraffic community. The controversy is typically focused on the utility of LRD models to predict the performance at network buffers. Our work here is not meant to advocate one modeling approach over another, but to point out (experimentally and theoretically) to the caveats in using the VT test as a tool for detecting LRD. We do that by deriving simple analytical expressions for the slope of the aggregated variance in three autocorrelated traffic models: M/G/&#8734; process (short-range dependent (SRD) but non-Markovian), the discrete autoregressive of order one model (SRD Markovian), and the fractional ARIMA process (LRD). Our main result is that the VT test often indicates, falsely, the existence of an LRD structure (i.e., H&gt;0.5) in synthetically generated traces from the two SRD models. The bias in the VT test, however, diminishes monotonically with the length of the trace. We provide some guidelines on selecting the minimum trace length so that the bias is negligible

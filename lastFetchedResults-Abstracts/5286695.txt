The minimum classification error (MCE) and maximum mutual information (MMI) objectives for discriminative training in automatic speech recognition and natural language processing are analyzed and compared theoretically. The results show that both objectives are related to posterior probability and error rates, and the MCE objective is more general and flexible than the MMI objective. The relations between the objectives and parameter optimization methods are also discussed. The results can help in understanding the discriminative objectives, in developing new objectives, and in discovering new training algorithms jointly with objectives.

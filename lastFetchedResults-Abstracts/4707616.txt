In generating sensor-based behavior of the robot, which has various sensors and many degrees of freedom, the key point is how it utilizes information from many sensors in multi modals for moving its body flexibly. In our experimental systems for generating multi-sensor guided behavior, we have introduced the following methods: 1) Integration with reflexes: Integrating objective behavior (behavior which is aimed for achieving some specific goals) and various kinds of reflexes (behavior which instantly reacts to the changes in an environment).; 2) Posture-sensor data memory: Memorizing the robot postures, relating them with changes of various sensors. This memory can also be used for predicting human intention from previous experiences.; and 3) ABC-Net (attention-based conditional network): Behavior description network, where nodes are expressed by states of sensors paid attention to and arcs are expressed by actions to transit between nodes. In those methods, behavior experiments for making sure the effectiveness have been done in both real and virtual environment using Kenta, a multi-DOF and multi-sensor humanoid that we have developed. In this paper, the design and implementation of the whole behavior systems including the above three methods for realizing the multi-sensor guided behavior are described and the results of the behavior experiments using Kenta are shown.

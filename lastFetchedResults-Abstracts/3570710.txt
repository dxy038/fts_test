This paper presents a deterministic solution to an approximated classification-error-based objective function. In the formulation, we propose a quadratic approximation as the function for achieving smooth error counting. The solution is subsequently found to be related to the weighted least-squares, whereby a robust tuning process can be incorporated. The tuning traverses between the least- squares estimate and the approximated total-error-rate estimate to cater to various situations of unbalanced attribute distributions. By adopting a linear parametric classifier model, the proposed classification-error-based learning formulation is empirically shown to be superior to that using the original least-squares-error cost function. Finally, it will be seen that the performance of the proposed formulation is comparable to other classification-error-based and state-of-the-art classifiers without sacrificing the computational simplicity.

Often adaptive, distributed control can be viewed as an iterated game between independent players. The coupling between the playersÂ´ mixed strategies, arising as the system evolves, is determined by the system designer. Information theory tells us that the most likely joint strategy of the players, given a value of the expectation of the overall control objective function, is the minimizer of a Lagrangian function of the joint strategy. So the goal of the system designer is to speed evolution of the joint strategy to that Lagrangian minimizing point, lower the expected value of the control objective function, and repeat. Here, we discuss how to do this using local descent procedures, and thereby achieve efficient, adaptive, distributed control.

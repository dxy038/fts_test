We enhance the Multi layer Perceptron to map a feature vector not only from the original d-dimensional feature space, but from an intermediate implicit Hilbert feature space in which kernels calculate inner products. The kernel substitutes the usual inner product between weight vectors and the input vector (or the feature vector of the hidden layer). The objective is to boost the generalization capability of this universal function approximator even more. Classification experiments with standard Machine Learning data sets are shown. We are able to improve the classification accuracy performance criterion for certain kernel types and their intrinsic parameters for the majority of the data sets.

In this article, a non-revisiting particle swarm optimization (NrPSO) is proposed NrPSO is an integration of the non-revisiting scheme and a standard particle swarm optimization (PSO). It guarantees that all updated positions are not evaluated before. This property leads to two advantages: 1) it undisputedly reduces the computation cost on evaluating a time consuming and expensive objective function and 2) It helps prevent premature convergence. The non-revisiting scheme acts as a self-adaptive mutation. Particles genericly switch between local search and global search. In addition, since the adaptive mutation scheme of NrPSO involves no parameter, comparing with other variants of PSO which involve at least two performance sensitive parameters, the performance of NrPSO is more reliable. The simulation results show that NrPSO outperforms four variants of PSOs on optimizing both uni-modal and multi-modal functions with dimensions up to 40. We also illustrate that the overhead and archive size of NrPSO are insignificant. Thus NrPSO is practical for real world applications. In addition, it is shown that the performance of NrPSO is insensitive to the specific chosen values of parameters.

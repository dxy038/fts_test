It is well known that the Hopfield Model (HM) for neural networks to solve the Traveling Salesman Problem (TSP) suffers from three major drawbacks. (1) It can converge on nonoptimal locally minimum solutions. (2) It can converge on infeasible solutions. (3) Results are very sensitive to the careful tuning of its parameters. A number of methods have been proposed to overcome (a) well. In contrast, work on (b) and (c) has not been sufficient; techniques have not been generalized to more general optimization problems. Thus this paper mathematically resolves (b) and (c) to such an extent that the resolution can be applied to solving with some general network continuous optimization problems including the Hopfield version of the TSP. It first constructs an Extended HM (E-HM) that overcomes both (b) and (c). Fundamental techniques of the E-HM lie in the addition of a synapse dynamical system cooperated with the current HM unit dynamical system. It is this synapse dynamical system that makes the TSP constraint hold at any final states for whatever choices of the IIM parameters and an initial state. The paper then generalizes the E-HM further to a network that can solve a class of continuous optimization problems with a constraint equation where both of the objective function and the constraint function are nonnegative and continuously differentiable

This study addresses an optimal computing power (node's service rates) allocation method to multiple processes (servers). The major objective of this paper is to present an analytic model for allocating computing power to input and output processes to minimize the mean response time (sum of mean node delay and mean transmission delay) of data requests at each node of a distributed database system.
The system under study consists of nodes connected to each other by virtual circuits within a communication network, such as a packet switching network. Each node (a host computer or a set of host computers) consisting of input and output M/M/1 processes has a unique database. The nodes and communication network form a product-form queuing network. The Lagrangian method is used for solving our model which allocates the processing capacity to M/M/1 processes.


Self-organisation is frequently advocated as the solution for managing large, dynamic systems. Distributed algorithms are implicitly designed for infinitely large problems, while small systems are regarded as being controllable using traditional, centralised approaches. Many real-world systems, however, do not fit conveniently into these &#8220;small&#8221; or &#8220;large&#8221; categories, resulting in a range of cases where the optimal solution is ambiguous. This difficulty is exacerbated by enthusiasts of either approach constructing problems that suit their preferred control architecture. We address this ambiguity by building an abstract model of task allocation in a community of specialised agents. We are inspired by the problem of work distribution in distributed satellite systems, but the model is also relevant to the resource allocation problems in distributed robotics, autonomic computing and wireless sensor networks. We compare the behaviour of a self-organising, market-based task allocation strategy to a classical approach that uses a central controller with global knowledge. The objective is not to prove one mechanism inherently superior to the other, instead we are interested in the regions of problem space where each of them dominates. Simulation is used to explore the trade-off between energy consumption and robustness in a system of intermediate size, with fixed communication costs and varying rates of component failure. We identify boundaries between regions in the parameter space where one or the other architecture will be favoured. This allows us to derive guidelines for system designers, thus contributing to the development of a disciplined approach to controlling distributed systems using self-organising mechanisms.

In this demonstration paper, we present a novel framework for searching objects (e.g., images, videos, etc.) captured by the users in a mobile social community. Our framework, is founded on an in-situ data storage model, where captured objects remain local on their owners smartphones and searches then take place over a novel lookup structure we compute dynamically. Initially, a query user invokes a search to find an object of interest. Our structure concurrently optimizes several conflicting objectives (i.e., it minimizes energy consumption, minimizes search delay and maximizes query recall), using a Multi-Objective Optimization approach and calculates a diverse set of high quality non-dominated Query Routing Trees (QRTs), in a single run. The optimal set is then forwarded to the query user (decision maker) to select a particular QRT to be searched based on instant requirements and preferences. To demonstrate the capabilities of SmartP2P during the conference, we will utilize our cloud of smartphone devices, i.e. the SmartLab testbed composed of 40+ Android smartphones and tablets, as well as mobility and social patterns derived by Microsofts Geolife project, DBLP and Pics n Trails. We will allow the attendees to use a real SmartLab Android device to query our local Smartphone Network using any of the four algorithmic choices provided by the SmartP2P framework. The query device will then be provided with the optimal QRTs and the attendees will be able to visually decide the optimal QRT to be searched. A P2P search on the Smartphone Network will follow making available to the query user the desired objects of interest, in an optimal manner. The conference attendees will be able to appreciate how social content can be efficiently shared with other attendees within close proximity without revealing their personal content to a centralized authority.

Efficient use of exa-scale parallel architectures will require applications to display a tremendous degree of concurrency in order to effectively employ hundreds of thousands to millions of cores. Large scale simulations of partial differential equations typically rely on a spatial domain decomposition approach, where the number of concurrent tasks is limited by the size of the spatial simulation domain. Time parallelism offers a promising approach to increase the degree of concurrency. Parareal is a popular, non-intrusive, iterative parallel in time algorithm that uses both low and high accuracy numerical solvers. While the high accuracy solutions are computed in parallel, the low accuracy ones are serial, which considerably hinders PararealÂ´s scalability, and therefore its potential usefulness in exa-scale environments. This paper proposes a nonlinear optimization approach to exploiting time parallelism. Like in the traditional Parareal approach, the time interval is partitioned into subdomains, and local time integrations are carried out in parallel. The objective cost function quantifies the mismatch of local solutions between adjacent time subintervals. The optimization problem is solved iteratively using gradient-based methods. The necessary gradients, and Hessian-vector products, involve only ideally parallel computations and are therefore highly scalable. Thus the proposed approach has the potential to make time parallelism an essential ingredient for exa-scale applications. The feasibility of the proposed algorithm is studied in the context of WRF (Weather Research \\&amp;amp; Forecast), a large-scale numerical weather prediction model. The derivative information required for optimization is obtained with the help of adjoint models. Implementation details and benefits of the new approach are discussed.

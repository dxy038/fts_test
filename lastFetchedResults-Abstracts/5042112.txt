In this paper, we propose a novel computation model for solving the distributed optimization problem where the objective function is formed by the sum of convex functions available to individual agent. Our approach differentiates from the existing approach by local convex mixing and gradient searching in that we force the states of the model to the global optimal point by controlling the subgradient of the global optimal function. In this way, the model we proposed does not suffer from the limitation of diminishing step size in gradient searching and allows fast asymptotic convergence. The model also shows robustness to additive noise, which is a main curse for algorithms based on convex mixing or consensus.

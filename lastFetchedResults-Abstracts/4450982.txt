Shared memory and message passing are two opposing communication models for parallel multicomputer architectures. Comparing such architectures has been difficult, because applications must be hand-crafted for each architecture, often resulting in radically different sources for comparison. While it is clear that shared memory machines are currently easier to program, in the future, programs will be written in high-level languages and compiled to the specific parallel target, thus eliminating this difference. The authors evaluate several parallel architecture alternatives, message passing, NUMA, and cache-coherent shared memory, for a collection of scientific benchmarks written in C*, a data-parallel language. Using a single suite of C* source programs, they compile each benchmark and simulate the interconnect for the alternative models. The objective is to examine underlying, technology-independent costs inherent in each alternative. The results show the relative work required to execute these data parallel programs on the different architectures, and point out where some models have inherent advantages for particular data-parallel program styles

This paper deals with the problem of objective evaluation of dynamic, multi-sensor image fusion. For this purpose an established static image fusion evaluation framework, based on gradient information preservation between the inputs and the fused image, is extended to deal with additional scene and object motion information present in multi-sensor sequences. In particular formulations for dynamic, multi-sensor information preservation models are proposed to provide space-time localised fusion performance estimates. Perceptual importance distribution models are derived to accommodate temporal data and provide a natural generalisation of localised performance estimates into both global and continuous dynamic fusion performance scores. The proposed system is described in detail and shown to exhibit better evaluation accuracy, robustness and sensitivity when compared to existing dynamic fusion metrics on an evaluation of several established image fusion algorithms applied to multi- sensor sequences from an array of dynamic fusion scenarios.

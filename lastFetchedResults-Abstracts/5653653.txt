Dynamic neural field theory has become a popular technique for modeling the spatio-temporal evolution of activity within the cortex. When using neural fields the right balance between excitation and inhibition within the field is crucial for a stable operation. Finding this balance is a severe problem, particularly in face of experience-driven changes of synaptic strengths. Homeostatic plasticity, where the objective function for each unit is to reach some target firing rate, seems to counteract this problem. Here we present a recurrent neural network model composed of excitatory and inhibitory units which can self-organize via a learning regime incorporating Hebbian plasticity, homeostatic synaptic scaling, and self-regulatory changes in the intrinsic excitability of neurons. Furthermore, we do not define a neural field topology by a fixed lateral connectivity; rather we learn lateral connections as well.

In Support Vector Regression (SVR), kernel functions are used to deal with nonlinear problem by computing the inner product in a higher dimensional feature space. The performance of approximation depends on the chosen kernels. Although the radial basis function (RBF) kernel has been successfully used in many problems, it still has the restriction in some complex problems. In order to obtain a more flexible kernel function, the non-negative weighting linear combination of multiple RBF kernels is used Then, the evolutionary strategy (ES) is applied for adjusting the parameters of SVR and kernel function. Moreover, the objective function of the ES is carefully designed, by involving a stability of bounded SVR. This leads to improved generalization performances and avoids the overfitting problem. The experimental results show the ability of the proposed method on symmetric mean absolute percentage error (SMAPE) that outperforms the other objective functions and grid search.

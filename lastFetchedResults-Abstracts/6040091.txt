Finding sparse approximate solutions to large underdetermined linear systems of equations is a common problem in signal/image processing and statistics. Basis pursuit, the least absolute shrinkage and selection operator (LASSO), wavelet-based deconvolution and reconstruction, and compressed sensing (CS) are a few well-known areas in which problems of this type appear. One standard approach is to minimize an objective function that includes a quadratic (<i>pound</i> <sub>2</sub>) error term added to a sparsity-inducing (usually <i>pound</i> <sub>1</sub>) regularizer. We present an algorithmic framework for the more general problem of minimizing the sum of a smooth convex function and a nonsmooth, possibly nonconvex, sparsity-inducing function. We propose iterative methods in which each step is an optimization subproblem involving a separable quadratic term (diagonal Hessian) plus the original sparsity-inducing term. Our approach is suitable for cases in which this subproblem can be solved much more rapidly than the original problem. In addition to solving the standard <i>pound</i> <sub>2</sub> - <i>pound</i> <sub>1</sub> case, our approach handles other problems, e.g., <i>pound</i> <sub>p</sub> regularizers with p ne 1, or group-separable (GS) regularizers. Experiments with CS problems show that our approach provides state-of-the-art speed for the standard <i>pound</i> <sub>2</sub> - <i>pound</i> <sub>1</sub> problem, and is also efficient on problems with GS regularizers.

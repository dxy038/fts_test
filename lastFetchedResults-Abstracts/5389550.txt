Support vector machine (SVM) is gaining much popularity as a powerful machine learning technique. SVM was originally developed for pattern classification and later extended to regression. One of main features of SVM is that it generalizes the maximal margin linear classifiers into high dimensional feature spaces through nonlinear mappings defined implicitly by kernels in the Hilbert space so that it may produce nonlinear classifiers in the original data space. On the oilier hand, the authors developed a family of various SVMs using multi-objective programming and goal programming (MOP/GF) techniques. This paper extends the family of SVM for classification to regression, and discusses their performance through numerical experiments.

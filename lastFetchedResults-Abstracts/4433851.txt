There is a large literature on the rate of convergence problem for general stochastic approximations, for algorithms where the step size either goes to zero or is small and constant. With the exception of the large deviations type, the rate of convergence work is essentially confined to the case where the limit point is not on a constraint boundary. The usual steps are hard to carry out when the limit point is on the boundary of the constraint set. The stability methods which are used to prove tightness of the normalized iterates cannot be carried over in general. We develop the necessary techniques and show that the stationary Gaussian diffusion is replaced by an appropriate stationary reflected linear diffusion. The rate of convergence results immediately imply the advantages of iterate averaging. An application to constrained function minimization under inequality constraints is given where both the objective function and the constraints are observed in the presence of noise

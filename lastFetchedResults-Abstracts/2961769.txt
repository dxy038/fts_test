This paper proposes the optimization of a non-standard objective function in the framework of maximum mutual information estimation (MMIE). In contrast with the classical MMIE estimation, where only misrecognized training utterances contribute to the optimization process, the contributions of near-miss classifications are naturally embedded in the maximization of the proposed function because it takes into account a non-linear combination of the probabilities of the competing models that can be tuned by means of a single parameter. This corrective training procedure has been applied to an isolated word recognition task leading to significant performance improvements with respect to maximum likelihood estimation and MMIE

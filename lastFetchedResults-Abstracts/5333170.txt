Due to the weaknesses of Neural Network (NN) learning, this paper proposes an alternative approach in enhancing NN learning by integrating improved cost function with control adaptation of the nodes and address memory. As commonly known, weight adjustments of NN particularly in Back propagation (BP) algorithm, involve the connections between neurons, the activation function used by the neurons, the learning algorithm that specifies the procedure for adjusting the weights and the cost functions. The cost functions of BP are calculated based on the derivatives. These derivatives will determine the success rate of the application to train the network with an error function that resembles the objective of the problem on hand. Due to that, the concept of weights governance with control part mechanism between the input and hidden layer, and unit offsets of the hidden layer are implemented. to alleviate the problems of BP learning. The address memory part of the network will detain the output pattern of the hidden layer. Subsequently, the output patterns are compared with the input pattern, and propels back to the output layer after learning. From the experiments, we found that the results are promising with these mechanisms and improved cost function which yields faster convergence rates.

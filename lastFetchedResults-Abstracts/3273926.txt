This doctoral consortium paper outlines the author´s proposed investigation into the use of the voice-source waveform for affective computing. A data-driven glottal waveform representation, previously examined in the authors earlier doctoral studies for its speaker discriminative abilities, is proposed to be studied for both depression detection and emotion recognition, including severity classification when considering depression. ´Data-driven´ refers to a parameterisation focus on the small but consistent idiosyncrasies of the glottal wave rather than only the mean shape and ratio measures. A review of the literature is given covering existing studies of the glottal waveform for depression detection and emotion classification. The benefits of developing easily accessible automatic recognition systems is stressed. The value of developing objective tools for clinicians in diagnosing depression is also conveyed. Finally research questions are framed and experimental methodologies discussed in order to address these. The studies proposed here will expand the body of knowledge regarding the information content of the glottal waveform and aim to improve depression detection and emotion classification accuracies based on the voice-source alone.

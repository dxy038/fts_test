Differential learning for statistical pattern classification is based on the classification figure-of-merit (CFM) objective function. It is proved that differential learning is asymptotically efficient, guaranteeing the best generalization allowed by the choice of hypothesis class as the training sample size grows large, while requiring the least classifier complexity necessary for Bayesian (i.e., minimum probability-of-error) discrimination. Differential learning almost always guarantees the best generalization allowed by the choice of hypothesis class for small training sample sizes

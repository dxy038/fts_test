In this paper we develop a LMS algorithm that converges to the global minimum of the mean square output error (MSE) objective function. This is accomplished by estimating the gradient as a smoothed version of the MSE. The smoothed MSE objective function begins as a convex functional. A cooling schedule is then applied such that over time it becomes the true MSE as the algorithm converges to the global minimum. We show that this smoothing process is achieved by convolving the objective function with a Gaussian probability density function, resulting in the LMS algorithm with a variable source appended to it. Simulation studies indicate that the proposed method consistently converges to the global minimum. We have shown a performance improvement over the IIR-LMS algorithm and the Steiglitz-McBride algorithm

Information divergence that measures the difference between two nonnegative matrices or tensors has found its use in a variety of machine learning problems. Examples are Nonnegative Matrix/Tensor Factorization, Stochastic Neighbor Embedding, topic models, and Bayesian network optimization. The success of such a learning task depends heavily on a suitable divergence. A large variety of divergences have been suggested and analyzed, but very few results are available for an objective choice of the optimal divergence for a given task. Here we present a framework that facilitates automatic selection of the best divergence among a given family, based on standard maximum likelihood estimation. We first propose an approximated Tweedie distribution for the &#946;-divergence family. Selecting the best &#946; then becomes a machine learning problem solved by maximum likelihood. Next, we reformulate &#945;-divergence in terms of &#946;-divergence, which enables automatic selection of &#945; by maximum likelihood with reuse of the learning principle for &#946;-divergence. Furthermore, we show the connections between &#947;- and &#946;-divergences as well as Renyi- and &#945;-divergences, such that our automatic selection framework is extended to non-separable divergences. Experiments on both synthetic and real-world data demonstrate that our method can quite accurately select information divergence across different learning problems and various divergence families.

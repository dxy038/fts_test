In this paper, a novel general robust SVM approach for classification is proposed which can better characterize the distribution of the data compared with the traditional SVM. The classical Support Vector Machines is heavily relied on the Support Vector which has neglected the holistic distribution of the data that sometimes will lead to gross errors. We first take use of the majority of the data distribution. Then we apply the distance between the two classes and modify the original SVM objective function. In the last step we search the best weights to make a balance between the distance and the maximum margin. We developed an improved method for optimizing over the SVM algorithm. Experiments on 10 famous data sets clearly demonstrate improved performance and this method is also readily applicable for data sets with different distribution.

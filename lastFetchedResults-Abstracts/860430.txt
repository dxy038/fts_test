A very simple gradient only algorithm for unconstrained minimization is proposed that, in terms of storage requirement and computational efficiency, may be considered as an alternative to the conjugate gradient line search methods for large problems. The method effectively applies the steepest descent method to successive simple (spherical) quadratic approximations of the objective function in such a way that no explicit line searches are performed in solving the minimization problem. It is shown that the method is convergent when applied to general positive-definite quadratic functions. The method is tested by its application to some standard and other test problems. On the evidence presented, the new method, called the SQSD algorithm, appears to be reliable and stable, and very competitive compared to the well-established Fletcher-Reeves and Polak-Ribiere conjugate gradient methods. In particular, it does very well when applied to extremely ill-conditioned problems.

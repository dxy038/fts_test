This paper proposes a new nonlinear optimization algorithm for the construction of radial basis function (RBF) networks in modelling nonlinear systems. The main objective is to speed up the learning convergence of the conventional conjugate gradient method. All the hidden layer parameters of RBF networks are simultaneously optimized by the conjugate gradient method while the output weights are adjusted accordingly using the orthogonal least squares (OLS) method. The derivatives used in the conjugate gradient algorithm are efficiently computed using a recursive sum squared error criterion. Numerical examples show that the new method converges faster than the previously proposed continuous forward algorithm (CFA).

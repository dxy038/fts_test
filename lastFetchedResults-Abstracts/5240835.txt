Summary form only given. We focus our attention on neural networks whose objective is hypothesis testing. The term hypothesis testing refers to a very broad class of problems, including classification, detection and pattern recognition. There is not much doubt about the way we can measure the performance of a classifier or detector. When the prior probabilities are known, we use the probability of error. When no priors are given, and the hypothesis testing problem is binary, we express the performance in terms of the power and false alarm probabilities. Unfortunately, the existing learning algorithms do not seem to have a lot in common with these performance criteria. We develop two new classes of learning algorithms, specially designed for hypothesis testing and applied to feed forward binary-output neural networks. The first class of algorithms provides optimization in the Neyman-Pearson sense. The second class deals with the probability of error or arbitrarily defined cost functions, and optimizes the network in the Bayesian sense

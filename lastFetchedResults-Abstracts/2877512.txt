SO-MODS is a new algorithm that combines surrogate global optimization methods with local search. SO-MODS is an extension of prior algorithms that sought to find near optimal solutions for computationally very expensive functions for which the number of allowable evaluations is strictly limited. The global search method in SO-MODS perturbs the best point found so far in order to find a new sample point. The number of decision variables being perturbed is dynamically adjusted in each iteration in order to be more effective for higher dimensional problems. The procedure for dynamically changing the dimensions perturbed is drawn from earlier work on the DYCORS algorithm. We use a cubic radial basis function as surrogate model and investigate two approaches to improve the solution accuracy. The numerical results show that SO-MODS is able to reduce the objective function value dramatically with just a few hundred evaluations even for 30-dimensional problems. The local search is then able to reduce the objective function value further.

In real-world visual communications, it is a common experience that end-users receive video with significantly time-varying quality due to the variations in video content/complexity, codec configuration, and network conditions. How human visual quality-of-experience (QoE) changes with such time-varying video quality is not yet well-understood. To investigate this issue, we conduct subjective experiments designed to examine the quality predictability between individual video segment of relatively constant quality and combined video consisting of multiple segments that have significantly different quality. Our data analysis suggests that simple models that pool segment-level quality, such as linear averaging and weighted-averaging, nonlinear min- and median-filtering, and distortion-weighted averaging, are limited in predicting the overall human quality assessment of the combined video. We thus propose a quality adaptation model that is asymmetrically tuned to increasing and decreasing quality. The proposed asymmetric adaptation (AA) model leads to improved performance of both subjective and objective quality assessment approaches when using segment-level quality scores to predict multi-segment time-varying video quality. The video database together with the subjective data will be made available to the public.

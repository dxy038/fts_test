Lack of knowledge of the prior probabilities in Bayesian process classifications from short sequences, may make temporary inferences unstable, or difficult to interpret. In some time-critical applications the use of uniform priors may be just too strong, or unjustified. A promising approach to &#8220;objective&#8221; prior determination is the application of the principle of maximum entropy to the model. The resulting so-called entropic priors, are applied here to Bayesian process classification with inferences based only on likelihood knowledge. We address the posterior consistency problem and derive a condition for ergodicity. The result is applied here to the classification of Gaussian processes. Some typical simulations of classification of AR processes are included.

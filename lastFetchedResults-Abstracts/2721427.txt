The development of adaptive filters is usually based either on a stochastic approximation of the gradient vector and the Hessian matrix, or on a deterministic minimization of quadratic a posteriori output errors. This paper investigates the design of adaptation algorithms by means of a minimum-disturbance approach together with added constraints. More than just rewriting objective functions minimized by the algorithms, the approach provides insight and extra tools for optimizing with respect to other parameters, e.g., the convergence factor &#956;. Designing new algorithms by adding extra costs or constraints to the objective function follows naturally, whereas the main characteristics of the algorithms remain clear. Understanding subtleties that set similar algorithms apart is also made easier. We apply the method to known algorithms, such as the LMS and RLS algorithms, and also to their variants. Rather than proposing a new algorithm, we hope this article will facilitate the development of different algorithms to meet the challenges posed by demanding applications. In addition, ensuing discussions may help understanding better the behavior of each algorithm in a particular scenario.

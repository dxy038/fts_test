In order to efficiently select a discriminative and complementary subset from a large feature pool, we propose a two-stage learning strategy considering both samples and their features simultaneously, namely sample selection and feature selection. The objective functions of both stages are consistent with a large margin loss. At the first stage, the support samples are selected by Support Vector Machine (SVM). At the second stage, a Boosting-like Sparsity Regularization (SRBoost) algorithm is presented to select a small number of complementary features. In detail, a weak learner is composed of a few features, which are selected by a sparsity enforcing mode, and an intermediate variable is gracefully used to reweight the corresponding sample. Extensive experimental results on the CASIA-IrisV4.0 database demonstrate that our method outperforms the state-of-the-art methods.

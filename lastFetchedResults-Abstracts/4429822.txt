Gene regulatory networks (GRN) inference is an important bioinformatics problem in which the gene interactions need to be deduced from gene expression data, such as microarray data. Feature selection methods can be applied to this problem. A feature selection technique is composed by two parts: a search algorithm and a criterion function. Among the search algorithms already proposed, there is the exhaustive search where the best feature subset is returned, although its computational complexity is unfeasible in almost all situations. The objective of this work is the development of a low cost parallel solution based on GPU architectures for exhaustive search with a viable cost-benefit. CUDA&#8482; is a general purpose parallel architecture with a new parallel programming model allowing that the NVIDIA<sup>&#174;</sup> GPUs solve complex problems in an efficient way. We developed a parallel algorithm for GRN inference based on the GPU/CUDA and encouraging speedups (60x) were achieved when assuming that each target gene has two predictors. The idea behind the proposed method can be applied considering three or more predictors for each target gene as well.

One of the mayor limitations associated to Neural Networks (NNs) learning algorithms is the number of input-output pattern pairs needed to obtain the desired classilication pcrlbmancc. In many practical cases, the number of pairs used to design the NN is much lower than necessary. Either bccause of the excessivc price of data acquisition or, simply, for availability reasons. An Importance Sampling (IS) technique is applied in this paper to NN training a in order to drastically improve the training cfticieiicy of the given training patterns. For this purpose, the Mean Square Error (MSE) objective function of a Backpropagalion algorithm has adequately been moditied hy applying a suboptimal IS fiinction. The application of the IS fiinction accelerates training convergence whereas it inaintains NN perromance.

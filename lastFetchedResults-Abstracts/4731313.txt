Due to the "generative" nature of the macro phenomena, agent-based systems require experience from the modeler to determine the proper low-level agent behavior. Adaptive and learning agents can facilitate this task: Partial or preliminary learnt versions of the behavior can serve as inspiration for the human modeler. Using a simulation process we develop agents that explore sensors and actuators inside a given environment. The exploration is guided by the attribution of rewards to their actions, expressed in an objective function. These rewards are used to develop a situation-action mapping, later abstracted to a human-readable format. In this contribution we test the robustness of a decision-tree-representation of the agent\Â´s decision-making process with regards to changes in the objective function. The importance of this study lies on understanding how sensitive the definition of the objective function is to the final abstraction of the model, not merely to a performance evaluation.

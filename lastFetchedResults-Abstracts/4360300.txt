Presents a learning scheme for training multilayer perceptrons (MLPs) with improved generalization ability. The algorithm employs a training algorithm based on a multi-objective optimization mechanism. This approach allows balancing between the training squared error and the norm of the network weight vector. This balancing is correlated with the trade-off between overfitting and underfitting. The method is applied to classification and regression problems and also compared with weight decay, support vector machines and standard backpropagation results. The proposed method leads to training results that are the best ones, and additionally allows a systematic procedure for training neural networks, with less heuristic parameter adjustments than the other methods

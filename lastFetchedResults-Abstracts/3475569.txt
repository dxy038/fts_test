At each instant of time we are required to sample a fixed number <img alt="m \\geq 1" src="/images/tex/3254.gif"/> out of <img alt="N" src="/images/tex/88.gif"/> i.i.d, processes whose distributions belong to a family suitably parameterized by a real number <img alt="\\theta" src="/images/tex/1471.gif"/> . The objective is to maximize the long run total expected value of the samples. Following Lai and Robbins, the learning loss of a sampling scheme corresponding to a configuration of parameters <img alt="C = (\\theta_{1},..., \\theta_{N})" src="/images/tex/3270.gif"/> is quantified by the regret <img alt="R_{n}(C)" src="/images/tex/3256.gif"/> . This is the difference between the maximum expected reward at time <img alt="n" src="/images/tex/23.gif"/> that could be achieved if <img alt="C" src="/images/tex/27.gif"/> were known and the expected reward actually obtained by the sampling scheme. We provide a lower bound for the regret associated with any uniformly good scheme, and construct a scheme which attains the lower bound for every configuration <img alt="C" src="/images/tex/27.gif"/> . The lower bound is given explicitly in terms of the Kullback-Liebler number between pairs of distributions. Part II of this paper considers the same problem when the reward processes are Markovian.

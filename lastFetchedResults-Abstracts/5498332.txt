Most gradient-based optimization methods involve a major difficulty-the derivation of the objective function. Usually, a large number of objective functions cannot be derived and, frequently, are not even continuous. A method for computing the network output sensitivities with respect to input variations for multilayer perceptrons (MLP) using differentiable activation functions is presented. The method is applied to obtain the expressions of the first and second order sensitivities. These sensitivities with the conjugated gradient can be used as a basis for the process optimization. As an illustration, the minimization of losses in a power system is presented. In this paper, three controls are used, namely the voltages at the generation buses, the positions of the transformers taps and the reactive power of switchable capacitors/inductors

SRAM is vulnerable to device-to-device variation (DDV), since it uses minimum-sized devices and requires device matching. In addition to the as-fabricated DDV at time-zero, aging induces a time-dependent DDV (TDDV). Bias temperature instability (BTI) is a dominant aging process. A number of techniques have been developed to characterize the BTI, including the conventional pulse-(I) -(V) , random telegraph noises, time-dependent defect spectroscopy, and TDDV accounting for the within-device fluctuation. These techniques, however, cannot be directly applied to SRAM, because their test conditions do not comply with typical SRAM operation. The central objective of this paper is to develop a technique suitable for characterizing both the negative BTI (NBTI) and positive BTI (PBTI) in SRAM. The key issues addressed include the SRAM relevant sensing Vg, measurement delay, capturing the upper envelope of degradation, sampling rate, and measurement time window. The differences between NBTI and PBTI are highlighted. The impact of NBTI and PBTI on the cell-level performance is assessed by simulation, based on experimental results obtained from individual devices. The simulation results show that, for a given static noise margin, test conditions have a significant effect on the minimum operation bias.

The main objective of this paper is to compare two adaptive algorithms for estimating the time delay between two-sensor outputs. These methods are very similar in the sense that they minimize the mean-squared error using a least mean square (LMS) gradient approach to estimate the time difference of arrival to two sensors. However, they apply the LMS gradient method in different ways, thereby holding different properties. Some theoretical aspects are addressed. Also, they are compared via computer simulations for a variety of cases, which include time-varying delay functions corresponding to moving source or receivers.

As a computer technology develops, high-end computing environments such as virtual reality (VR) are no longer limited to data display in their applications. We focus on simulations and real-time explorations of simulated models, which lead to the situation where we can no longer rely on simple mechanical devices such as the keyboard, mouse and joystick that are commonly used in human-computer interaction. Foot-mounted input gesture detection is a spin-off project from the demand we have encountered in working on creative projects in a VR environment. The objective was to develop an interface that accounts for one of the most basic human movements, natural stance and bipedal locomotion. Unlike previous walking interfaces, such as sensor tiles, treadmills and steppers, our device is not limited to a fixed position since it is wearable in free motion. Further, the multiplicity of pressure signals from the foot provides a high-dimensional control source that is inherent in the design, while the modularity of the signals provides a means for differentiating human-determined motion patterns. Pattern recognition was implemented using rule-based inference based on fuzzy logic

In this paper, we present a new method for optimizing support vector machines for regression problems. This algorithm searches for efficient feasible directions. Within these selected directions, we choose the best one, i.e. the one, coupled with an optimal step analytical evaluation, that ensures a maximum increase of the objective function. The resulting solution, the gradient and the objective function are recursively determined and the Gram matrix has not to be stored. Our algorithm is based on SVM-Torch proposed by Collobert for regression, which is similar to SVM-Light suggested by Joachims for classifications problems, but adapted to regression problems. We are also inspired by LASVM proposed by Bordes for classification problems. F-SVR algorithm uses a new efficient working set selection heuristic, ingeniously exploits quadratic function properties, so it is fast as well as easy to implement and is able to perform on large data sets.

In the current practice, the performance evaluation of RF-based indoor localization solutions is typically realized in non-standardized environments and following ad-hoc procedures, which hampers objective comparison and does not provide clear insight into their intrinsic properties. Many evaluation procedures also neglect important environmental factors like RF interference, diminishing the real-world value of the obtained results. Localization competitions, in which different solutions are evaluated along a set of standardized metrics under unified and representative conditions, can play an important role in mitigating these problems, but their organization is cost and labor intensive. In this paper we report on the design, execution and results from an online localization competition in which several different RF-based indoor localization algorithms have been evaluated with the help of a remotely accessible and automated testbed infrastructure, that reduces these overheads. The competing algorithms have been evaluated following a combination of precision, latency and sensitivity metrics, under four different benchmarking scenarios, resulting in 28 different benchmarking experiments. The obtained results provide strong indication that specific types of RF-interference noticeably degrade the localization performance.

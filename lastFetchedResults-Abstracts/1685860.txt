We consider a distributed convex optimization problem over a network where multiple agents collectively try to minimize a sum of local convex functions of the same variables, each of which is available to one specific agent only. For solving this optimization problem, we present an inexact version of the dual averaging method. This extends recent results of Duchi (2012), which cover the error-free case, to the case where an error is present in calculating the subgradient of the objective function or in computing the projection. We show that when the errors decrease at appropriate rates, our method achieves the same convergence rate as in the error-free case. In particular, the convergence of the method is also established for nonsummable errors. We also provide numerical results to validate the theoretic results.

In multi-sensor distributed estimation fusion, local estimation errors are correlated in general. Two extreme ways to handle this correlation is either to ignore them completely or to have them fully considered. There is another case in the middle: it admits the existence of the correlation, but does not know how large it is. A sensible way is to set up an optimality criterion and optimize it over all possible such correlations. This work is a new development in the third class. First, a new general objective function is introduced, which is the minimum sum of statistical distances between the fused density and the local posterior densities. Then it is shown that the new criterion leads to a convex optimization problem if the Kullback-Leibler (KL) divergence is used as the statistical distance between assumed Gaussian densities. It is found that although the analytically obtained fused estimate using the new criterion differs from the simple convex combination rule only in mean squared error (MSE) by a scaling factor N (the number of sensors used), it is pessimistic semi-definite in MSE. Numerical examples illustrate the effectiveness of the proposed distributed fuser by comparing with several widely used distributed fusers.

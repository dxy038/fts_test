This project aims at developing an automated framework for depression detection. During a depressive episode, patients suffer from psychomotor retardation and this phenomenon is not only limited to facial activity. In this PhD work, it is hypothesized that such complex affective state can be better represented by integrating information from various uni-modal channels to form a multimodal affective sensing system. The project explores facial dynamics, body expressions such as head movement, relative body part movement etc. in patients with major depressive disorders. The contribution of various channels is assessed and as a final objective, a framework combining discriminative channels for automatic depression analysis is proposed.

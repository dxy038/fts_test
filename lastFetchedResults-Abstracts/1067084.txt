Although artificial neural networks (ANNs) have proven to be useful tools for modeling many aspects of the hydrological cycle, the fact that they do not provide any means of exploiting fundamental knowledge of the system means that they are still viewed with some skepticism. In this paper, an approach is presented for incorporating information about relative input contributions in the development of an ANN during the calibration and validation stages. Two case studies are presented which highlight the uncertainty associated with calibrating and validating an ANN based on predictive error alone and demonstrates the necessity of constraining the calibration of an ANN to ensure physical plausibility. The proposed technique was used in the comparison of three training algorithms in terms of their ability to find a globally optimal solution and it was identified that neither in-sample nor out-of-sample performance measures are very informative about the solutions obtained, nor do they necessarily indicate that a reasonable approximation of the underlying relationship has been achieved. It was shown that by applying constraints to the objective function, an ANN could be developed with physically plausible input contributions and comparable predictive performance to that of an unconstrained model. A sensitivity analysis was carried out to verify the proposed methodology.

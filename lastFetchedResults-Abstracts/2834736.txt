In this paper we present an automatic and self-organized Reinforcement Learning (RL) based approach for Cell Outage Compensation (COC). We propose that a COC module is implemented in a distributed manner in the Enhanced Node Base station (eNB)s in the scenario and intervenes when a fault is detected and so the associated outage. The eNBs surrounding the outage zone automatically and continually adjust their downlink transmission power levels and find the optimal antenna tilt value, in order to fill the coverage and capacity gap. With the objective of controlling the intercell interference generated at the borders of the extended cells, a modified Fractional Frequency Reuse (FFR) scheme is proposed for scheduling. Among the RL methods, we select a Temporal Difference (TD) learning approach, the Actor Critic (AC), for its capability of continuously interacting with the complex wireless cellular scenario and learning from experience. Results, validated on a Release 10 Long Term Evolution (LTE) system level simulator, demonstrate that our approach outperforms state of the art resource allocation schemes in terms of number of users recovered from outage. Index Terms-Self-Organizing Network (SON), Self Healing, Reinforcement Learning, LTE/LTE-Advanced, COC.

Multiple outliers are frequently encountered in regression models used in business, economics, engineers and applied studies. The ordinary least squares (OLS) estimator fails even in the presence of a single outlying observation. To overcome this problem, a class of high breakdown robust estimators (insensitive to outliers up to 50% of the data sample) has been introduced as an alternative to the least squares regression. Among them the Penalized Trimmed Squares (PTS) is a reasonable high breakdown estimator. This estimator is defined by the minimization of an objective function where penalty cost for deleting an outlier is added, which serves as an upper bound on the residual error for any feasible regression line. Since the PTS does not require presetting the number of outliers to delete from the data set, it has better efficiency with respect to other estimators. However, small outliers remain influential causing bias to the regression line. In this work we present a new class of regression estimates called generalized PTS (GPTS). The new GPTS estimator is defined as the PTS but with penalties suitable for bounding the influence function of all observations. We show with some numerical examples and a Monte Carlo simulation study that the generalized PTS estimate has very good performance for both robust and efficiency properties.

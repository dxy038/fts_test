A computer program (the Audit Assistant) was developed to help physicians review the care of critically ill emergency department (ED) patients. The program is an example of a new class of decision aids that serves to remind physicians to consider possibilities, not an artificial intelligence program that actually attempts to simulate clinical reasoning. The goal of such programs is to enable physicians to reduce errorsâ€”in this case to enable reviewers to notice more of the errors in care in the cases they are reviewing. The objective of this study was to demonstrate on a small set of complex cases that the tested computer program enables the physician to perform a better quality review. The issue of what constitutes improved case review is addressed. In the first part of the study, reviewers reviewed two mock charts without using the Audit Assistant and then immediately reviewed the charts again with the assistance of the program. The reviews were compared. In the second part of the study, a second reviewer also compared the utility of the review of the first reviewer alone and the Audit Assistant output as an aid to review, using an additional mock chart. Six emergency physicians participated; each was a quality assurance director for the ED of one Cleveland area hospital. For the physicians reviewing without the Audit Assistant, 41% of critical actions were listed by three or four reviewers. For those using the Audit Assistant, 83% of critical actions were listed by three or four reviewers. All reviewers preferred the Audit Assistant-suggested list to the critical action list generated by a previous reviewer not using the Audit Assistant (P< .02). Use of the Audit Assistant improved the completeness and the consistency of physician review of mock charts of critically ill ED patients in a small series of cases. The critical actions added for review were important, as demonstrated by the preferential addition of critical actions chosen by other reviewers who were not using the computer program.

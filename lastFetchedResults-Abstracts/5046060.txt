The classical expectation-maximization (EM) algorithm for image reconstruction suffers from particularly slow convergence when additive background effects such as accidental coincidences and scatter are included. In addition, when smoothness penalties are included in the objective function, the M-step of the EM algorithm becomes intractable due to parameter coupling. The authors describe the space-alternating generalized EM (SAGE) algorithm, in which the parameters are updated sequentially using a sequence of small &#8220;hidden&#8221; data spaces rather than one large complete-data space. The sequential update decouples the M-step, so the maximization can typically be performed analytically. By choosing hidden-data spaces with considerably less Fisher information than the conventional complete-data space for Poisson data, the authors obtain significant improvements in convergence rate. This acceleration is due to statistical considerations, not to numerical overrelaxation methods, so monotonic increases in the objective function and global convergence are guaranteed. Due to the space constraints, the authors focus on the unpenalized case in this summary, and they eliminate derivations that are similar to those in Lange and Carson, J. Comput. Assist. Tomography, vol. 8, no. 2, p.306-16 (1984)

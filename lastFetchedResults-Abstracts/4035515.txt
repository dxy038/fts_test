We address the problem of estimating automatically from audio signals the similarity between two pieces of music, a technology that has many applications in the online digital music industry. Conventional methods of audio music search use distance measures between features derived from the audio for this task. We describe three techniques that make use of music classifiers to derive representations of audio features that are based on culturally motivated information learned by the classifier. When these representations are used for similarity estimation, they produce very significant reductions in computational complexity over existing techniques (such as those based on the KL-divergence), and also produce metric similarity spaces, which facilitate the use of technologies for the sub-linear scaling of search times. We have evaluated each system using both pseudo-objective techniques and human listeners, and we demonstrate that this efficiency gain is obtained while providing a comparable level of performance when compared with existing techniques.

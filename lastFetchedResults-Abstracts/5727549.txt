We consider regularized covariance estimation in scaled Gaussian settings, e.g., Elliptical distributions, compound-Gaussian processes and spherically invariant random vectors. The classical maximum likelihood (ML) estimate due to Tyler is asymptotically optimal under different criteria and can be efficiently computed even though the optimization is non-convex. We propose a unified framework for regularizing this estimate in order to improve its finite sample performance. Our approach is based on the discovery of hidden convexity within the ML objective, namely convexity on the manifold of positive definite matrices. We regularize the problem using appropriately convex penalties. These allow for shrinkage towards the identity matrix, shrinkage towards a diagonal matrix, shrinkage towards a given positive definite matrix, and regularization of the condition number. We demonstrate the advantages of these estimators using numerical simulations.

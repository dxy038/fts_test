Monaural speech separation is one of the most difficult problems in speech signal processing. In this paper, a new method based on machine learning and computational auditory scene analysis (CASA) is suggested to separate the monaural speech of two-talker. The technique of machine learning is used to learn the grouping cues on isolated clean data from single speaker. By using a factorial-max vector quantization model (MAXVQ) to infer the masking signals needed in resynthesis, the objective of separation is accomplished. The results of experiment on a standard corpus show that this proposed method could separate the mixed speech of two speakers very well. The SNR of the separated speech are improved obviously.

Decision support approaches for software development frequently rely on the ability of experts to measure subjective attributes consistently on an ordinal scale. Examples include: decision approaches concerning alternative software architectures that are needed to optimise maintenance effort; approaches to achieve development goals which rely on Bayesian Belief Networks; pricing decisions for maintenance contract tendering that use direct measurement of maintainability; and pricing decisions concerning effort estimation that require measurement of `complexityÂ´ attributes. In addition the validation of prediction systems and objective indirect measures for subjective attributes (e.g. maintainability, cohesion) require that observers can directly measure the attributes consistently. However, intuition and some anecdotal evidence suggest that during modular effort estimation there may be module effects that lead to under estimation. A Bayesian inference procedure can enable an assessment of whether the consistency of measurement of a modular attribute may be influenced by module effects. For example, whether the chance of correctly classifying a modular attribute might vary with module length. The study examines two data sets, one taken from a cohesion experiment and the other for a maintainability experiment. In so doing, evidence that module length interacts with the chance of correctly classifying maintainability and cohesion is inferred. These interactions show that it is necessary for those who undertake direct measurement of modular attributes to be made aware of the potential of unsolicited module effects to influence measurement consistency

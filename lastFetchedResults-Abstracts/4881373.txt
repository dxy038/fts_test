With the rapid development of text summarization, evaluation methods for automatic Chinese text summarization system are becoming more and more important in natural language processing, which can promote development of text summarization greatly. This paper analyzes the existed methods for automatic summarization evaluation, and introduces a new evaluation method based on cluster. The main idea of new method is to compare automatic summaries with original text directly by counting the number of sub topics in the original text which the content of automatic summaries can cover, so it is not requiring model summaries. We know that because model summaries have so much personal views of the writer, one of the main defects of traditional methods using model summary is subjectivity. The new method can avoid the subjective factor influence of model summary. The original tests have shown that this new method saves time and labour force. Most importantly, it is an objective process, so the scores are more convinced to every one.

Traditionally, the calibration of groundwater models has depended on gradient-based local optimization methods. These methods provide a reasonable degree of success only when the objective function is smooth, second-order differentiable, and satisfies the Lipschitz's condition. For complicated and highly nonlinear objective functions it is almost impractical to satisfy these conditions simultaneously. Research in the calibration of conceptual rainfall-runoff models, has shown that global optimization methods are more successful in locating the global optimum in the region of multiple local optima. In this study, a global optimization technique, known as shuffle complex evolution (SCE), is coupled to the gradient-based Lavenbergâ€“Marquardt algorithm (GBLM). The resultant hybrid global optimization algorithm (SCEGB) is then deployed in parallel testing with SCE and GBLM to solve several inverse problems where parameters of a nonlinear numerical groundwater flow model are estimated. Using perfect (i.e. noise-free) observation data, it is shown SCEGB and SCE are successful at identifying the global optimum and predicting all model parameters; whereas, the commonly applied GBLM fails to identify the optimum. In subsequent inverse simulations using observation data corrupted with noise, SCEGB and SCE again outperform GBLM by consistently producing more accurate parameter estimates. Finally, in all simulations the hybrid SCEGB is seen to be equally effective as SCE but computationally more efficient.

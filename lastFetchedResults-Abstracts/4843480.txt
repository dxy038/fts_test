There exist optimization problems with the target objective, which is to be optimized, and several extra objectives, which can be helpful in the optimization process. The EA+RL method is designed to control optimization algorithms which solve problems with extra objectives. The method is based on the use of reinforcement learning for adaptive online selection of objectives. In this paper we investigate whether OneMax helps to optimize LeadingOnes when the EA+RL method is used. We consider LeadingOnes+OneMax problem where the target objective is LeadingOnes and the only extra objective is OneMax. The following theoretical results are proven for the expected running times when optimization starts from a random vector in the case of randomized local search (RLS): n<sup>2</sup>/2 for LeadingOnes, n<sup>2</sup>/3 for LeadingOnes+OneMax when reinforcement learning state is equal to the LeadingOnes fitness or when random objective selection is performed, and n<sup>2</sup>/4+o(n<sup>2</sup>) when there is one reinforcement learning state and the greedy exploration strategy is used. The case of starting with all bits set to zero is also considered. So, OneMax helps, although not too much, to optimize LeadingOnes when RLS is used. However, it is not true when using the (1 + 1) evolutionary algorithm, which is shown experimentally.

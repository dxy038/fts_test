The multiple training concept first applied to bidirectional associative memory training is applied to the one-sweep back-propagation algorithm. The algorithm is called multiple training back-propagation. Computer simulations show that by putting different weights on different pairs in the energy function, this algorithm can increase the training speed of the network. The pair weights are updated during the training phase using the basic differential multiplier method. However, those pair weights are not used during the decoding phase. A sufficient condition for convergence of the training phase is provided, followed by two simulation examples, XOR and stochastic test

This paper proposes some new algorithms for unconstrained optimization problems, which have been obtained by application of control theory called direct gradient descent control. A static optimization problem is solved with a dynamic controller by which the convergence speed can be accelerated to a great extent. The main idea is to consider an objective function F(x) and its time derivative <sup>dF(x(t))</sup>/<sub>dt</sub> as a performance criterion of control and to apply a gradient descent method. We then obtain several new optimization algorithms which use the second order derivative (Ilessian) Fxx(x(t)) but not its inverse as the Newton method does. It is confirmed by simulations that the proposed methods possess very excellent convergence property to an optimum. It is also interesting that our methods have a function of finding not a local but a global optimum to some extent

The quantitative mapping of a database that represents a finite set of classified and/or unclassified data points may be decomposed into three distinctive learning tasks: (1) detection of the structure of each class model with locally mixture clusters; (2) estimation of the data distributions for each induced cluster inside each class; and (3) classification of the data into classes that realizes the data memberships. The mapping function accomplished by the probabilistic modular networks may then be constructed as the optimal estimator with respect to information theory, and each of the three tasks can be interpreted as an independent objective in real-world applications. We adapt a model fitting scheme that determines both the number and kernel of local clusters using information-theoretic criteria. The class distribution functions are then obtained by learning generalized Gaussian mixtures, where a soft classification of the data is performed by an efficient incremental algorithm. Further classification of the data is treated as a hard Bayesian detection problem, in particular, the decision boundaries between the classes are fine tuned by a reinforce or antireinforce supervised learning scheme. Examples of the application of this framework to medical image quantification, automated face recognition, and featured database analysis, are presented as well
